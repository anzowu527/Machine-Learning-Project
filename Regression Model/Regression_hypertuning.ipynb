{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PLZHz3R_mF2b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lHcD1qFPmPIy"
   },
   "outputs": [],
   "source": [
    "train_x4 = pd.read_csv(\"train_x4.csv\")\n",
    "train_y4 = pd.read_csv(\"train_y4.csv\")\n",
    "test_x2 = pd.read_csv(\"test_x2.csv\")\n",
    "test_x = pd.read_csv(\"test_X.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [3566, 7838], 1: [2848, 3086, 7247, 7397], 2: [270, 2299, 9369], 3: [5570, 7158], 4: [466, 6860], 5: [1876, 2468], 6: [6853, 6915, 7206], 7: [1499, 2603], 8: [5684, 9474], 9: [801, 7432], 10: [162, 474, 2463, 2486, 7825], 11: [1322, 8723], 12: [8996, 9014], 13: [1331, 4358, 4485, 5244, 6385], 14: [322, 5378, 8079], 15: [4653, 6373, 9089], 16: [1762, 5455, 6282, 7932, 9165], 17: [414, 5951], 18: [1248, 8892, 8933], 19: [145, 3393], 20: [8666, 9179], 21: [5057, 7987], 22: [4103, 5996], 23: [1844, 1861], 24: [2281, 8648], 25: [6121, 6289, 7710, 8703], 26: [3759, 6723], 27: [3037, 5595], 28: [1423, 5285], 29: [6564, 9331], 30: [2231, 2305, 3712], 31: [1056, 1809, 5528, 7818], 32: [1005, 1988, 2449, 6897], 33: [3171, 3657, 6847], 34: [46, 5231], 35: [4351, 5635], 36: [3011, 8618], 37: [5347, 6906], 38: [2941, 3841], 39: [3605, 3829], 40: [3015, 9424], 41: [1692, 2056], 42: [2456, 3054], 43: [884, 9361], 44: [3218, 4889, 6052, 6474, 6501, 6603], 45: [1630, 2367], 46: [1175, 6856], 47: [7084, 7109], 48: [4803, 9163], 49: [859, 2270], 50: [4003, 6486], 51: [256, 2822], 52: [1673, 3133], 53: [506, 1632, 2365, 2706, 3708, 5472, 9119], 54: [2211, 6405], 55: [1681, 7055], 56: [7075, 7285], 57: [92, 115], 58: [317, 374, 2756, 8742, 8753], 59: [2193, 6513, 7417, 8567, 9065], 60: [1467, 5924], 61: [1286, 9023], 62: [4439, 9121], 63: [3287, 5014], 64: [3323, 7152], 65: [2787, 2998, 6170], 66: [3522, 6277, 8348], 67: [2238, 7892, 8473], 68: [7185, 7381], 69: [967, 7631], 70: [3563, 8597], 71: [1118, 2718], 72: [5702, 8001], 73: [8162, 9210], 74: [7380, 8852], 75: [2585, 9173], 76: [25, 5678], 77: [4654, 8477], 78: [1146, 5928], 79: [3275, 8266], 80: [9070, 9401], 81: [5868, 6699], 82: [4628, 6852], 83: [2192, 8390], 84: [1677, 7765], 85: [1170, 7989], 86: [23, 527], 87: [3071, 7301], 88: [1108, 2149], 89: [2757, 5876], 90: [4535, 4725], 91: [2000, 5392], 92: [2799, 7113], 93: [3298, 7464], 94: [745, 6022], 95: [7035, 7166], 96: [2986, 4723], 97: [1715, 4085], 98: [1814, 3868], 99: [106, 4342], 100: [6099, 6850], 101: [5734, 9102, 9508], 102: [2759, 8868], 103: [5226, 6566, 8123], 104: [2036, 7554], 105: [5257, 9117], 106: [1128, 3748], 107: [201, 403, 1287, 3627, 6344], 108: [4235, 5460], 109: [1939, 5782], 110: [5095, 6854], 111: [5539, 6317], 112: [539, 1991], 113: [128, 4231, 4252, 6113, 6236, 6414, 8024, 8211, 8673], 114: [5828, 7038], 115: [477, 1963], 116: [3714, 8896], 117: [737, 2537], 118: [3084, 6710, 6781], 119: [6140, 9622], 120: [3504, 9362], 121: [1302, 2260, 3394], 122: [2790, 9528], 123: [77, 383, 1877, 2020, 2481, 2627, 3475, 4692, 6051, 6746, 7172], 124: [4560, 8235], 125: [5908, 6637, 7142, 7209, 9541], 126: [1052, 1109, 1775, 5009, 5477], 127: [1066, 1671, 1833, 2794, 3651, 5397, 6008], 128: [1195, 3533, 5159, 5426, 6925, 8110], 129: [2870, 5147, 8346], 130: [4650, 7190], 131: [515, 951, 1519, 2685, 3244, 3285, 4423, 4668, 5263, 6659, 9256], 132: [8325, 9046], 133: [2156, 4162], 134: [299, 8076], 135: [1215, 9605], 136: [4913, 5723], 137: [7933, 9534], 138: [8959, 9172], 139: [1053, 1837, 7817, 8568], 140: [503, 3729, 7333, 8170], 141: [6098, 6838, 8513], 142: [4837, 6996, 7270], 143: [2610, 3376], 144: [5642, 7370], 145: [834, 5906, 6435, 6536, 7133], 146: [5527, 6254], 147: [2116, 9028], 148: [3529, 8533], 149: [1422, 2582, 3453, 9075], 150: [7337, 7490], 151: [3750, 6214], 152: [1716, 1912, 3778, 6354, 8985], 153: [1344, 4026], 154: [2453, 2477], 155: [2913, 3167], 156: [1709, 4740], 157: [2279, 2467], 158: [5508, 7037], 159: [30, 868], 160: [7347, 9050], 161: [3260, 7369], 162: [1149, 1705, 1914, 5990, 7894, 9349], 163: [1119, 5104, 7952], 164: [637, 4118, 6199, 7134], 165: [2683, 4496, 5214, 6271], 166: [142, 2460, 3215, 6580], 167: [8391, 8676, 9223], 168: [1165, 5195], 169: [1527, 3511], 170: [5757, 6009], 171: [1808, 4407], 172: [736, 1657, 6029, 9026], 173: [189, 505, 5422, 6952], 174: [2754, 4632, 7265, 7801, 9411], 175: [734, 2153], 176: [289, 2080], 177: [51, 5008], 178: [419, 9260, 9367], 179: [5459, 6026], 180: [7548, 8897], 181: [836, 7867], 182: [3136, 3189], 183: [3985, 4703], 184: [190, 7569], 185: [3626, 9456], 186: [1194, 5386], 187: [5970, 6719], 188: [6670, 8292], 189: [5331, 8883], 190: [2104, 4468, 6312], 191: [1329, 2663, 2746, 3009, 4053, 5428, 6903, 8698], 192: [4307, 4680, 5083, 6020, 8743, 9262], 193: [769, 6562], 194: [2140, 2425, 7485, 8642, 9056], 195: [95, 429, 1609, 4904, 5840, 7406], 196: [3572, 4042, 8059, 8641], 197: [65, 305, 460, 470, 981, 987, 1079, 2274, 2331, 2415, 2418, 2471, 2527, 2530, 2873, 2884, 2892, 2988, 3132, 3328, 3481, 3575, 3656, 3924, 3927, 3929, 3964, 4165, 4607, 4715, 5017, 5288, 5298, 5522, 5540, 5652, 5973, 6091, 6158, 6290, 6369, 6381, 6804, 7140, 7260, 7371, 7513, 7580, 7628, 7772, 7823, 8616, 8863, 9437], 198: [1335, 2866, 3070, 3283, 5352, 5783, 5838, 5890, 6114, 7557, 7641, 7731], 199: [2697, 4155], 200: [3642, 7168], 201: [972, 2771, 9306], 202: [5881, 6862], 203: [4714, 4756], 204: [5802, 6124], 205: [4, 3310, 6790], 206: [2543, 3448], 207: [5810, 7821], 208: [4835, 5463, 7703], 209: [140, 2291, 2575, 4254, 5049, 5857, 7893, 9317], 210: [1297, 4301, 7070, 7845], 211: [1753, 9134], 212: [5603, 6890, 9097], 213: [3052, 3476], 214: [1306, 5210], 215: [1185, 6624], 216: [1281, 7137, 8213], 217: [1748, 1993, 3030, 3332, 5538, 7965, 8558, 8663, 9059], 218: [2324, 6324], 219: [4138, 4690, 5088, 6085, 8758], 220: [227, 446, 4068], 221: [2202, 3484, 7663], 222: [1336, 6500], 223: [1557, 5967], 224: [3908, 5780, 6056], 225: [1784, 9323], 226: [438, 1343, 3797, 3802, 5284, 7946], 227: [9011, 9468], 228: [4498, 6427], 229: [4156, 4463], 230: [3580, 4172], 231: [269, 5907], 232: [292, 6327, 7751], 233: [812, 2088], 234: [3869, 5404], 235: [1761, 3389], 236: [2677, 3610, 9461], 237: [2751, 7246], 238: [1823, 8736], 239: [566, 946, 6905], 240: [156, 5785], 241: [7461, 7789], 242: [481, 1101, 6645], 243: [69, 3716, 9355], 244: [7610, 8267, 9326], 245: [1117, 6296], 246: [3008, 3051, 4589], 247: [739, 7616, 9465], 248: [31, 5946, 6481], 249: [2421, 3224], 250: [1966, 6537], 251: [6970, 8652], 252: [7201, 9290], 253: [4848, 6858], 254: [761, 1906, 5542, 6340], 255: [3378, 9484], 256: [4538, 8540], 257: [2416, 5278], 258: [866, 4330], 259: [3269, 8206], 260: [3228, 9104], 261: [1102, 3671], 262: [1036, 2671], 263: [3860, 6896], 264: [3564, 6468], 265: [758, 4109, 7318], 266: [4061, 5708], 267: [4370, 8385], 268: [572, 9636], 269: [5388, 5450], 270: [1253, 5728, 8755], 271: [2622, 5791], 272: [1836, 5452], 273: [1237, 2340, 5660, 6222, 7516, 8791], 274: [8440, 8997], 275: [2354, 7670], 276: [132, 2673], 277: [3336, 4667], 278: [3921, 6831], 279: [661, 7735], 280: [312, 8732], 281: [2087, 4961], 282: [1518, 8914], 283: [857, 6516], 284: [543, 3507], 285: [1349, 4735, 8651], 286: [7591, 9187], 287: [1377, 2737], 288: [7759, 8782], 289: [4557, 5364], 290: [1642, 4113], 291: [4292, 4863], 292: [6183, 9312], 293: [4523, 5359], 294: [4396, 5430], 295: [2115, 6767, 9196], 296: [1552, 9024], 297: [7611, 8275], 298: [1885, 6088], 299: [0, 7445], 300: [839, 8595], 301: [2102, 6988], 302: [3213, 7389], 303: [2547, 5758, 6179], 304: [2147, 2912], 305: [838, 3207], 306: [2360, 9212], 307: [3503, 7734], 308: [1832, 4798, 6889], 309: [1446, 1565, 2584, 3214, 9314, 9410], 310: [4816, 8015, 9276], 311: [7199, 7231], 312: [5318, 6251, 7693], 313: [434, 468, 1272, 3335, 3574, 6154, 9171], 314: [67, 3764, 3863], 315: [4998, 5423, 6339], 316: [2601, 2664], 317: [4721, 9080], 318: [402, 5718], 319: [3184, 7430], 320: [3912, 5882], 321: [4976, 9537], 322: [610, 870], 323: [4898, 6545], 324: [8620, 9123], 325: [1104, 6981], 326: [2014, 6901], 327: [5626, 7483], 328: [1427, 9374], 329: [764, 4771], 330: [3505, 6490], 331: [1945, 2316], 332: [4807, 7200], 333: [33, 8392], 334: [37, 6794], 335: [3551, 5031], 336: [830, 1121], 337: [243, 3495], 338: [232, 568, 2512], 339: [5461, 7587], 340: [3252, 5737], 341: [5814, 9124], 342: [550, 5886, 7561], 343: [8329, 9279], 344: [4707, 8032], 345: [2762, 8253], 346: [4592, 8364], 347: [7402, 8335], 348: [4678, 8638], 349: [492, 5383], 350: [6177, 6323], 351: [5700, 7053], 352: [3959, 7203, 8619], 353: [1311, 7747], 354: [273, 963, 983, 2510, 7563], 355: [3112, 9511], 356: [1718, 2388], 357: [343, 2037, 3824], 358: [878, 1477], 359: [6832, 7435], 360: [149, 1024, 1114, 4296, 7577, 7588], 361: [1220, 2322], 362: [217, 3915], 363: [373, 3062], 364: [6674, 7465], 365: [7906, 8556], 366: [3165, 7995], 367: [462, 7147], 368: [3770, 5683], 369: [8433, 9451], 370: [450, 7935], 371: [2723, 9092], 372: [2297, 4108, 9442], 373: [593, 7986], 374: [3095, 6673], 375: [8041, 9586], 376: [194, 9103], 377: [4336, 8675], 378: [2511, 8708], 379: [1954, 4509], 380: [188, 368, 1595, 7426], 381: [3782, 6145], 382: [2905, 8858], 383: [3907, 9490], 384: [3354, 8600], 385: [528, 7899], 386: [8518, 8761], 387: [1644, 2721, 4717], 388: [1204, 3571, 8777], 389: [2164, 2423, 7472], 390: [4306, 4492, 7940], 391: [1793, 4797], 392: [5141, 8564], 393: [4006, 7150], 394: [5915, 9606], 395: [3199, 6112], 396: [6429, 8546], 397: [3734, 3968, 4800, 6769], 398: [4022, 6053], 399: [3135, 9615], 400: [2218, 3775, 7473], 401: [787, 1494], 402: [3301, 9348], 403: [2954, 8488], 404: [6712, 7767, 8210], 405: [7477, 7791], 406: [3154, 3896], 407: [2011, 4446, 9392], 408: [3058, 5232, 5706, 6193], 409: [1918, 2278, 3875, 4095, 4962], 410: [1249, 6787], 411: [3877, 8323], 412: [3936, 6591], 413: [1406, 2764], 414: [1543, 3995], 415: [185, 9225, 9427], 416: [2268, 7960, 9602], 417: [1589, 2363], 418: [4049, 5175], 419: [2508, 3113], 420: [1555, 3206], 421: [4385, 7994], 422: [999, 8811], 423: [6768, 7401], 424: [3041, 7604], 425: [2867, 4051, 9101, 9203], 426: [882, 2945, 3056, 4080, 9496], 427: [5659, 7346], 428: [3954, 7101, 7471, 8034, 9426], 429: [4933, 5135, 6695], 430: [4276, 9441], 431: [2935, 4352, 4633, 4896, 9110], 432: [5811, 5918], 433: [1737, 9003], 434: [1878, 2028, 7044], 435: [514, 4183], 436: [3267, 4994, 8042], 437: [8561, 9336], 438: [682, 2490], 439: [3148, 6687], 440: [4143, 6301], 441: [975, 6715], 442: [5911, 7412], 443: [347, 1911, 2517, 2895, 3172, 6093, 6665, 7119], 444: [2755, 4271], 445: [2670, 5682], 446: [4720, 7359], 447: [3977, 5481], 448: [3725, 6413], 449: [2802, 3360], 450: [3552, 5261], 451: [2708, 2863, 8677], 452: [285, 2554], 453: [1672, 2780], 454: [2208, 5986], 455: [1731, 5290], 456: [742, 2827], 457: [483, 4537], 458: [5133, 7903], 459: [5323, 7019], 460: [4175, 7116], 461: [1443, 7520], 462: [3637, 6594], 463: [547, 1482, 5046], 464: [2256, 6235], 465: [847, 5727], 466: [9328, 9470], 467: [3102, 8784, 9181], 468: [3202, 5036], 469: [1926, 4689, 6521], 470: [4033, 5841], 471: [1704, 5674], 472: [3836, 8467], 473: [2405, 7165], 474: [687, 8945], 475: [1023, 6356], 476: [1667, 3311], 477: [2722, 8216], 478: [1347, 2043], 479: [3616, 6352], 480: [2050, 3033, 6264], 481: [697, 8257], 482: [2178, 4546], 483: [2621, 3421, 9574], 484: [934, 8923], 485: [6082, 6888], 486: [2075, 8602], 487: [365, 6130, 6487], 488: [2923, 7422], 489: [676, 2846], 490: [563, 9169], 491: [2991, 7324], 492: [3562, 4228, 8841], 493: [511, 3186, 3499, 4763, 5151, 5627, 6623, 7559, 7674, 9291, 9618], 494: [3719, 5506], 495: [329, 7852], 496: [4369, 6706, 6788, 6969], 497: [4137, 4585, 8865], 498: [5773, 6011], 499: [2518, 8832], 500: [2138, 7281], 501: [3672, 5618], 502: [2182, 6995], 503: [43, 1838], 504: [3956, 5161], 505: [6560, 8990], 506: [1690, 3299, 3904], 507: [318, 6007], 508: [1354, 1887, 5954, 7539], 509: [5898, 9498], 510: [455, 867], 511: [1588, 3045], 512: [1040, 5975], 513: [779, 2370], 514: [2077, 5764], 515: [1608, 7266], 516: [959, 2098], 517: [60, 2081, 5992, 7576], 518: [4047, 7363], 519: [2819, 5123], 520: [6439, 8705], 521: [7436, 8999], 522: [1944, 7529], 523: [5698, 8599], 524: [138, 253], 525: [3221, 4642, 8415], 526: [1240, 1916], 527: [1807, 2745, 4526], 528: [712, 8191], 529: [7942, 8215, 9219], 530: [5233, 6305], 531: [248, 5667], 532: [3201, 8572], 533: [3638, 5076], 534: [4440, 5061], 535: [2242, 6587], 536: [7746, 7887], 537: [5309, 7236, 7410], 538: [2190, 5417], 539: [4297, 4923, 5806], 540: [5563, 5602], 541: [1039, 8570], 542: [6514, 7632, 9108], 543: [1511, 2061], 544: [386, 3532], 545: [1374, 3830], 546: [240, 4905, 8109, 8773], 547: [3794, 7849], 548: [3945, 4857], 549: [426, 1542, 5112], 550: [2589, 6696], 551: [920, 2446, 2850], 552: [7487, 9096], 553: [4936, 9099], 554: [3953, 5592], 555: [6416, 6494], 556: [3975, 7898], 557: [6632, 9188], 558: [1920, 2813], 559: [4601, 8807], 560: [730, 9475], 561: [50, 3783], 562: [2630, 8418], 563: [463, 4101, 7080], 564: [1292, 3630, 4384], 565: [822, 2452], 566: [413, 9423], 567: [59, 716, 5819], 568: [1539, 3117], 569: [4663, 8083], 570: [9346, 9560], 571: [7736, 8332], 572: [1203, 2993, 4711, 5927], 573: [1687, 6485], 574: [479, 4427], 575: [2052, 2194], 576: [4286, 4506], 577: [3941, 8697], 578: [6255, 8516], 579: [6240, 8869], 580: [4139, 4250], 581: [3942, 5862], 582: [4256, 8165], 583: [619, 5981], 584: [1549, 4874], 585: [2396, 8856], 586: [3608, 8704], 587: [7579, 9034], 588: [811, 1124, 6469], 589: [367, 2042], 590: [360, 4615, 6089], 591: [2304, 7130], 592: [1971, 7345], 593: [1524, 5809, 9231], 594: [250, 1062], 595: [510, 1560, 2607], 596: [986, 3468], 597: [2180, 3525], 598: [6936, 7658], 599: [387, 974, 3579, 3589, 4291, 7117], 600: [1264, 1729, 2447, 2877, 3380, 4897, 4969, 5056, 6219, 6731, 7344, 7488, 7836, 8115, 8586, 9519], 601: [2021, 8061], 602: [8314, 9047], 603: [4603, 8454], 604: [5202, 6407], 605: [226, 8002, 9464], 606: [4765, 5442], 607: [4089, 5227], 608: [1251, 5645], 609: [2996, 8203], 610: [5179, 8242], 611: [6709, 8444], 612: [1177, 7833], 613: [3243, 4079], 614: [1426, 2329], 615: [4732, 6524, 7972], 616: [8577, 8836], 617: [1631, 5949], 618: [1820, 1983], 619: [6732, 6745], 620: [3527, 6810, 8057, 8086], 621: [2012, 2125], 622: [1898, 7532], 623: [652, 7296], 624: [5353, 9083], 625: [5801, 8485], 626: [714, 4753, 5647, 8369, 9365, 9554], 627: [1893, 2309, 5313], 628: [5051, 7571], 629: [2561, 5297], 630: [3367, 5266], 631: [2684, 5930], 632: [918, 1362], 633: [2109, 7645], 634: [3382, 5681, 9178], 635: [1585, 3837], 636: [1633, 7257], 637: [1514, 3813], 638: [301, 5921], 639: [1858, 3515], 640: [559, 7835], 641: [2187, 3002, 3621, 4037, 7458, 8189, 8954], 642: [45, 7530], 643: [2162, 6749, 7807], 644: [4522, 6803, 6960, 8483, 9136], 645: [7699, 8815], 646: [1064, 7678], 647: [5251, 9265], 648: [3838, 7441], 649: [2466, 6879, 8645], 650: [57, 7396], 651: [130, 5011], 652: [4831, 7056], 653: [2628, 3739], 654: [338, 2137, 5274], 655: [7511, 9460], 656: [2235, 6351], 657: [971, 6748], 658: [740, 3325, 4536], 659: [4219, 8771], 660: [692, 1535], 661: [1111, 2859, 6383], 662: [6245, 8297], 663: [982, 6037], 664: [1756, 5109], 665: [7357, 8078], 666: [3675, 8534], 667: [3749, 6399], 668: [3449, 8749], 669: [2139, 3732], 670: [7654, 7944], 671: [768, 2251], 672: [3846, 7202], 673: [6079, 8016], 674: [1724, 4757], 675: [1069, 1688], 676: [1547, 8105], 677: [3590, 8394], 678: [979, 1978, 5839], 679: [650, 7334], 680: [5593, 8023], 681: [2928, 7618], 682: [6688, 8289], 683: [2591, 5215], 684: [493, 8224, 9197], 685: [266, 6319, 9009], 686: [579, 2782, 4179, 6515, 8680], 687: [5765, 6045], 688: [5, 2151, 4626, 7897], 689: [3479, 8861], 690: [1987, 4786, 6902, 7339], 691: [4048, 7271], 692: [4564, 9274], 693: [3834, 4947], 694: [795, 9486], 695: [592, 4599], 696: [4178, 5145, 8813], 697: [816, 1867], 698: [1030, 7621], 699: [2009, 7662, 8839], 700: [508, 8219], 701: [100, 5427], 702: [1558, 3718, 4710], 703: [104, 5456], 704: [1444, 2715], 705: [4455, 6463, 6876], 706: [1145, 1856], 707: [2426, 5465], 708: [448, 7057, 7999], 709: [881, 2216], 710: [1375, 4308], 711: [2039, 7740], 712: [904, 5438], 713: [1592, 7212], 714: [2195, 7709], 715: [2307, 2326], 716: [3744, 6010, 8259], 717: [4856, 8000, 8125], 718: [2736, 8795], 719: [441, 6366, 6654], 720: [498, 5489], 721: [5113, 7073], 722: [3081, 7499], 723: [5843, 7444], 724: [5414, 6176], 725: [6361, 6825], 726: [473, 8127], 727: [671, 1686], 728: [7181, 8831], 729: [120, 2835], 730: [911, 9405], 731: [3387, 8395], 732: [1205, 6751], 733: [3713, 5833], 734: [954, 2661], 735: [1994, 2727], 736: [587, 4472], 737: [1663, 1981, 3338, 4985, 5164, 7291, 8197], 738: [2644, 7886], 739: [2141, 6980], 740: [80, 1174, 1288, 1787, 1790, 1879, 2469, 3603, 3817, 4687, 4814, 5216, 5498, 5565, 6149, 6642, 7727, 8529], 741: [3464, 5177, 9209], 742: [6270, 9216], 743: [10, 7030], 744: [549, 5217], 745: [1791, 2401, 6129], 746: [5363, 8873, 9430], 747: [5020, 8527], 748: [4182, 4948], 749: [1245, 5143, 5536], 750: [3661, 8217], 751: [3772, 6313], 752: [694, 1147, 1294, 2825, 3192, 3647, 4412, 5010, 6403], 753: [398, 3596, 4422, 6151], 754: [221, 6299], 755: [3698, 9603], 756: [4094, 4208]}\n",
      "{0: [6.4225, 6.0665], 1: [2.3866, 4.4113, 4.8067, 4.4037], 2: [6.7685, 6.774, 6.8254], 3: [5.0009, 5.1545], 4: [6.0607, 5.946000000000001], 5: [2.1365, 2.5946], 6: [3.6444, 3.5924, 6.7815], 7: [5.5311, 5.53], 8: [4.7206, 4.9502], 9: [3.2845, 3.2236], 10: [6.2535, 6.3224, 6.5389, 6.3143, 6.6303], 11: [2.352, 4.8262], 12: [5.2355, 4.8668], 13: [3.8114, 6.5969, 6.5497, 5.4772, 6.0742], 14: [1.9286, 3.7446, 2.6627], 15: [2.4589, 3.2346, 3.1727], 16: [5.0865, 3.6252, 4.6842, 5.8004, 5.7267], 17: [5.9006, 3.9304], 18: [2.6402, 3.9125, 4.9265], 19: [5.2079, 5.5101], 20: [2.5308, 2.7057], 21: [0.5482, 6.1137], 22: [5.5753, 5.6128], 23: [0.092, 5.4634], 24: [5.2422, 5.3244], 25: [6.4035, 6.1785, 6.1924, 6.36], 26: [6.1073, 1.4006], 27: [2.4832, 2.6117], 28: [4.7566, 5.0404], 29: [1.8438, 2.1733], 30: [0.1493, 5.0214, 3.3862], 31: [4.9218, 5.619, 5.4498, 5.4041], 32: [4.9599, 4.9449, 5.1497, 5.0876], 33: [2.9326, 3.2847, 0.7273], 34: [0.6341, 3.0119], 35: [2.8996, 2.8984], 36: [0.4453, 0.5374], 37: [1.6282, 1.7007], 38: [4.6052, 4.4633], 39: [4.1302, 3.3901], 40: [4.7038, 4.7066], 41: [1.3006, 3.1038], 42: [1.4088, 2.6051], 43: [4.4513, 1.0059], 44: [2.8943, 3.854, 0.7676, 2.8869, 3.6664, 3.7928], 45: [2.0961, 1.992], 46: [4.0288, 3.189], 47: [2.9261, 2.7848], 48: [3.5856, 2.9132], 49: [1.2197, 0.9983], 50: [3.4443, 3.3094], 51: [4.9035, 4.9002], 52: [1.977, 1.7253], 53: [5.3049, 4.9262, 2.992, 4.0034, 5.4006, 4.1742, 3.8929], 54: [5.0409, 3.7503], 55: [4.7955, 4.6688], 56: [6.1067, 4.5948], 57: [0.1344, 0.1714], 58: [4.8022, 4.8524, 5.0478, 4.7005, 4.852], 59: [4.3743, 5.3193, 5.3387, 5.3082, 4.4359], 60: [7.449, 7.2206], 61: [4.9026, 2.5509], 62: [3.362, 3.2313], 63: [3.1486, 2.3483], 64: [2.7705, 3.0325], 65: [0.5114, 0.2614, 0.3469], 66: [0.9325, 1.2158, 0.4624], 67: [0.5089, 2.7319, 2.8256], 68: [2.553, 2.8306], 69: [3.1755, 3.638], 70: [0.5745, 2.2553], 71: [0.4009, 2.7359], 72: [2.9751, 3.4467], 73: [0.8583, 0.9057], 74: [5.0889, 4.5538], 75: [3.9407, 3.3069], 76: [5.6809, 5.9779], 77: [0.3957, 0.6375], 78: [3.382, 2.9511], 79: [2.9548, 2.9782], 80: [7.3145, 7.3277], 81: [0.7969, 1.3456], 82: [3.7355, 3.7417], 83: [3.2488, 2.887], 84: [1.9485, 1.9233], 85: [0.7937, 0.8159], 86: [2.2137, 2.1077], 87: [3.3533, 3.3595], 88: [0.8947, 1.6333], 89: [3.965, 3.6102], 90: [3.6667, 3.6968], 91: [4.2447, 4.2242], 92: [6.8856, 0.1276], 93: [5.5011, 4.901], 94: [5.249, 5.2783], 95: [6.3265, 6.2376], 96: [4.239, 4.1293], 97: [2.5344, 2.4182], 98: [2.3069, 2.2636], 99: [2.6031, 2.597], 100: [2.9398, 3.0299], 101: [2.7617, 3.3160000000000003, 3.3306], 102: [3.8507, 5.4832], 103: [6.8531, 7.4546, 7.0887], 104: [3.5598, 5.1434], 105: [0.8869, 0.2179], 106: [3.1437, 0.9853], 107: [6.2814, 6.5248, 6.5855, 6.3539, 6.6506], 108: [0.1127, 2.6691], 109: [6.0394, 5.9228], 110: [6.6457, 5.3516], 111: [6.8659, 6.7059], 112: [1.6679, 2.1069], 113: [4.4558, 3.5412, 3.4888, 4.4571, 4.4564, 5.1279, 3.5744, 5.2143, 1.5681], 114: [6.2543, 6.2591], 115: [2.9001, 2.9109], 116: [5.0801, 5.1561], 117: [6.3724, 4.9374], 118: [1.1825, 1.1787, 1.1809], 119: [2.5965, 2.5893], 120: [5.9978, 6.0438], 121: [4.717, 4.2403, 4.716], 122: [1.5103, 2.8389], 123: [4.5708, 4.5299, 0.2261, 3.3312, 0.5246, 4.1177, 4.3718, 4.4042, 3.489, 2.4526, 0.2649], 124: [0.7997, 1.3537], 125: [0.8713, 2.891, 1.4461, 3.17, 2.478], 126: [1.0732, 0.1657, 3.5681, 2.3267, 4.6337], 127: [6.4639, 6.6284, 6.7006, 6.4626, 6.428999999999999, 6.598, 6.4682], 128: [2.027, 2.192, 1.9682, 1.6948, 2.2311, 2.4561], 129: [2.8645, 4.9582, 5.1336], 130: [0.821, 1.4366], 131: [2.2898, 2.0207, 1.776, 1.8118, 1.9687, 1.3705, 1.9594, 1.9904, 1.6254, 2.0173, 1.8546], 132: [4.537, 3.7221], 133: [4.2173, 3.7789], 134: [5.7867, 3.7439], 135: [4.7287, 5.5222], 136: [4.3222, 4.672], 137: [5.218999999999999, 4.931], 138: [3.2594, 2.7211], 139: [1.567, 1.4415, 1.9512, 3.0066], 140: [4.9988, 4.626, 4.2341, 4.958], 141: [1.9305, 1.9924, 1.5817], 142: [2.5943, 2.7542, 2.4282], 143: [4.1577, 4.5197], 144: [2.9371, 4.2431], 145: [1.7885, 0.6917, 1.2845, 0.9385, 1.7788], 146: [2.2684, 1.6223], 147: [3.9661, 3.9368], 148: [3.4254, 3.3923], 149: [2.6837, 1.9868, 3.2981, 3.2206], 150: [1.1274, 0.9221], 151: [4.5507, 3.0392], 152: [0.4149, 0.8183, 1.6383, 1.5543, 1.3155], 153: [2.8739, 3.0701], 154: [4.0849, 4.1714], 155: [4.1246, 4.6841], 156: [1.3776, 1.9074], 157: [0.3017, 0.9702], 158: [0.4497, 0.1782], 159: [0.4066, 0.3908], 160: [1.9168, 2.4459], 161: [3.7846, 3.7879], 162: [7.0233, 7.2733, 3.1761, 7.3545, 6.9901, 7.3873], 163: [1.9555, 2.8109, 0.995], 164: [4.0213, 0.8607, 4.0157, 3.9103], 165: [3.3058, 3.2727, 4.0639, 4.434], 166: [4.2469, 3.3298, 4.6527, 2.0304], 167: [5.6226, 5.6849, 5.0955], 168: [3.5507, 2.6031], 169: [3.4646, 2.5925], 170: [2.5018, 2.5044], 171: [2.5389, 2.2431], 172: [1.5228, 1.3649, 1.3797, 0.7722], 173: [0.6271, 1.7509, 1.9258, 1.0719], 174: [1.5553, 1.9379, 1.8125, 1.848, 1.3177], 175: [0.5931, 1.5055], 176: [1.9741, 0.8037], 177: [4.1719, 3.7404], 178: [2.0748, 0.6669, 0.1857], 179: [0.9767, 0.6969], 180: [1.3464, 1.7152], 181: [0.5506, 0.5614], 182: [2.7594, 1.4569], 183: [2.605, 2.3492], 184: [2.2327, 2.2230000000000003], 185: [3.4253, 3.5979], 186: [1.8906, 2.1167], 187: [2.0727, 2.9803], 188: [1.9969, 2.2255], 189: [0.6556, 2.154], 190: [1.3985, 1.6312, 1.2933], 191: [3.2585, 2.7381, 2.8856, 3.0749, 3.0106, 4.0268, 3.2951, 3.0123], 192: [4.2322, 2.6164, 4.8262, 4.255, 5.3128, 5.8491], 193: [4.4812, 4.3683], 194: [4.147, 4.646, 4.3922, 4.5912, 3.5538], 195: [4.6846, 4.6128, 4.624, 5.4138, 5.791, 4.6369], 196: [5.5489, 5.4363, 5.4565, 4.8447], 197: [0.1774, 5.6117, 4.042, 5.4406, 5.5364, 5.5298, 1.7313, 4.5555, 5.3667, 5.7004, 5.535, 5.6409, 4.1314, 5.4329, 5.5462, 5.2349, 4.5125, 5.1692, 5.3685, 5.6753, 5.6433, 4.8046, 4.3006, 5.4777, 5.2389, 5.228, 5.1397, 5.3076, 5.646, 5.3617, 6.1848, 2.5345, 5.5718, 5.5184, 5.6368, 1.1866, 4.9845, 5.7244, 2.2854, 5.0094, 1.3602, 5.519, 1.3842, 5.593999999999999, 4.0431, 5.4182, 2.7024, 2.7602, 5.4901, 0.3239, 5.3828, 5.5426, 5.5548, 4.398], 198: [4.6701, 5.3305, 5.6319, 4.9222, 4.9599, 5.4127, 3.1928, 5.3011, 5.483, 5.6441, 2.9236, 5.4841], 199: [4.9462, 5.7707], 200: [4.9826, 4.9966], 201: [4.4934, 5.2539, 5.4317], 202: [5.5696, 5.5441], 203: [4.5105, 4.5153], 204: [5.0059, 4.9904], 205: [2.9344, 4.005, 3.7583], 206: [5.5116, 5.169], 207: [4.9826, 4.379], 208: [5.6787, 3.1291, 5.681], 209: [3.5539, 3.5587, 3.6805, 4.7087, 3.6965, 4.7183, 3.6749, 4.8317], 210: [4.3227, 4.4098, 4.2616, 4.1563], 211: [5.4607, 5.5652], 212: [3.773, 4.2751, 3.7411], 213: [3.7523, 3.5478], 214: [2.8958, 2.8643], 215: [2.9758, 2.3822], 216: [2.7573, 2.9411, 2.1486], 217: [2.5721, 2.575, 3.1172, 1.8203, 2.2613, 1.8581, 2.2445, 2.2607, 2.8179], 218: [1.2101, 1.2759], 219: [2.3582, 2.8956, 2.6173, 2.7913, 2.6856], 220: [2.4112, 1.2691, 0.9152], 221: [1.7001, 1.8913, 1.7194], 222: [1.7565, 1.2805], 223: [3.0137, 3.2208], 224: [1.6664, 2.1274, 2.2625], 225: [1.0195, 1.2576], 226: [0.6847, 0.6769, 0.7303, 0.7439, 0.7171, 0.1723], 227: [2.3463, 2.4071], 228: [3.0096, 3.0448], 229: [1.8612, 2.1302], 230: [0.4035, 1.8984], 231: [2.7688, 2.8713], 232: [1.0412, 0.9115, 0.4767], 233: [0.3858, 0.8874], 234: [2.0844, 2.7002], 235: [0.467, 0.695], 236: [2.5076, 1.418, 2.3817], 237: [3.693, 3.5122], 238: [3.3816, 0.1274], 239: [0.7144, 1.4473, 0.7408], 240: [2.3737, 2.4047], 241: [1.6432, 1.6843], 242: [1.8735, 1.5838, 1.9805], 243: [1.3123, 0.2569, 1.2836], 244: [0.1569, 0.7822, 0.7621], 245: [1.6543, 0.5606], 246: [2.6999, 3.4443, 3.6497], 247: [1.378, 2.2135, 2.1843], 248: [1.4368, 3.2437, 1.9938], 249: [0.6854, 1.0851], 250: [1.2131, 3.9848], 251: [1.7921, 1.7922], 252: [1.1229, 1.0815], 253: [1.1492, 1.6311], 254: [1.2606, 1.3167, 0.1961, 0.1504], 255: [1.2241, 1.2226], 256: [0.9546, 0.9239], 257: [2.6414, 2.6391], 258: [3.2621, 3.9374], 259: [4.8011, 4.4502], 260: [4.3352, 4.3859], 261: [4.1079, 4.1053], 262: [2.4365, 2.4096], 263: [2.4948, 2.7295], 264: [1.9362, 1.8151], 265: [3.3701, 3.4546, 3.3209], 266: [1.6938, 1.8236], 267: [2.6529, 1.3674], 268: [1.0297, 1.0375], 269: [2.9902, 3.0137], 270: [2.8315, 3.6294, 2.4389], 271: [2.6532, 2.4006], 272: [2.8042, 2.7969], 273: [1.1805, 3.1365, 0.7382, 0.8869, 1.4607, 3.1721], 274: [1.0563, 0.8331], 275: [2.658, 1.8662], 276: [3.8743, 4.2633], 277: [3.9282, 3.9113], 278: [2.8333, 2.9499], 279: [1.3473, 1.4523], 280: [1.0486, 0.9756], 281: [2.03, 2.2305], 282: [1.1229, 1.0599], 283: [2.7375, 4.0208], 284: [3.7087, 3.9906], 285: [3.19, 3.442, 3.2676], 286: [0.8109999999999999, 1.1798], 287: [4.2852, 3.7493], 288: [0.7675, 0.8001], 289: [4.3843, 4.1051], 290: [5.2204, 5.4484], 291: [1.4538, 1.4529], 292: [1.2025, 1.2005], 293: [1.3655, 2.7879], 294: [2.3866, 2.2898], 295: [0.3074, 1.0055, 0.7974], 296: [3.8366, 3.6988], 297: [2.7438, 3.2756], 298: [4.8087, 4.6703], 299: [2.8121, 2.9587], 300: [0.6387, 0.2125], 301: [1.5619, 3.6084], 302: [4.7342, 3.0804], 303: [4.0566, 3.9541, 4.0902], 304: [2.7634, 2.7557], 305: [5.0983, 4.2949], 306: [4.4725, 4.096], 307: [4.1099, 3.7511], 308: [3.8508, 4.0416, 4.4621], 309: [3.4665, 2.8463, 3.1229, 3.8732, 3.5814, 4.0951], 310: [3.3247, 3.2126, 3.4733], 311: [2.4226, 0.7969], 312: [2.4798, 2.5507, 3.4717], 313: [2.3613, 1.8354, 1.8298, 2.1602, 1.8576, 2.7122, 2.3591], 314: [2.047, 2.2266, 1.7303], 315: [1.6401, 2.0802, 2.7687], 316: [0.7937, 1.653], 317: [2.2801, 2.0982], 318: [0.5094, 2.7444], 319: [3.0475, 3.0375], 320: [1.6769999999999998, 1.4383], 321: [2.1054, 1.9834], 322: [1.5224, 1.5011], 323: [2.0462, 1.9915], 324: [0.4668, 0.4867], 325: [2.4016, 2.4723], 326: [1.7273, 1.66], 327: [1.9685, 1.6023], 328: [3.3781, 3.64], 329: [1.2391, 1.2508], 330: [2.0481, 2.7578], 331: [1.4735, 1.6939], 332: [2.4375, 2.4628], 333: [0.9417, 0.9406], 334: [0.7077, 0.5522], 335: [1.3736, 1.4665], 336: [1.4968, 1.504], 337: [1.4517, 1.4461], 338: [2.9966, 1.6058, 2.2218], 339: [2.4303, 2.3197], 340: [3.4275, 3.3074], 341: [2.8445, 3.1405], 342: [1.5193, 1.5382, 1.3597], 343: [3.4888, 3.1643], 344: [2.0808, 2.0786], 345: [3.0315, 2.8952], 346: [0.4387, 0.4514], 347: [0.9617, 1.1607], 348: [1.0847, 1.0826], 349: [2.1552, 2.26], 350: [2.9496, 0.6925], 351: [0.2248, 0.3207], 352: [1.4249, 0.3295, 1.1654], 353: [2.6471, 2.2524], 354: [0.5238, 1.7677, 0.3236, 0.6375, 0.6819], 355: [1.279, 1.4776], 356: [2.6745, 1.1259], 357: [2.0906, 1.9032, 0.4271], 358: [0.6067, 1.0898], 359: [2.145, 2.4613], 360: [2.9789, 2.7888, 2.8038, 2.6234, 2.382, 2.8877], 361: [1.5179, 1.8334], 362: [2.4617, 2.8659], 363: [1.5316, 1.6457], 364: [3.1925, 3.2698], 365: [3.2775, 3.2789], 366: [2.5652, 2.4387], 367: [4.2828, 2.117], 368: [1.7531, 1.7697], 369: [1.5855, 1.5829], 370: [3.8251, 4.1487], 371: [4.4642, 4.437], 372: [2.1721, 2.2097, 3.1856], 373: [2.9251, 2.9151], 374: [1.4275, 1.4326], 375: [2.1329, 3.0389], 376: [4.332, 4.1767], 377: [1.5443, 1.5429], 378: [1.5964, 1.5475], 379: [3.6584, 3.6586], 380: [2.1373, 0.3636, 2.1377, 2.1862], 381: [2.6385, 2.9979], 382: [1.9648, 0.6679], 383: [1.4486, 1.4566], 384: [0.9794, 0.9897], 385: [3.6879, 3.4819], 386: [1.844, 1.6055], 387: [2.7528, 2.3603, 2.091], 388: [2.9455, 1.3832, 5.3868], 389: [3.0583, 0.9875, 3.2797], 390: [3.1484, 3.2442, 3.6562], 391: [2.0795, 2.0658], 392: [3.3659, 3.3902], 393: [4.3309, 4.7356], 394: [1.2998, 1.2695], 395: [1.1044, 1.5863], 396: [3.7707, 3.5263], 397: [2.3029, 4.5932, 4.9523, 2.733], 398: [0.9186, 2.3983], 399: [3.9482, 0.103], 400: [2.8836, 2.8185, 0.2946], 401: [4.0571, 3.2112], 402: [3.1333, 2.8736], 403: [2.4716, 3.3352], 404: [3.4163, 3.37, 1.2232], 405: [0.7174, 0.7707], 406: [0.1823, 0.1399], 407: [5.5739, 2.0055, 2.4605], 408: [3.805, 3.5927, 3.7953, 3.6154], 409: [5.5238, 3.6914, 4.2453, 2.7529, 2.9626], 410: [2.9647, 2.962], 411: [0.6547, 0.1931], 412: [3.6548, 3.6578], 413: [4.1375, 3.8995], 414: [3.992, 3.9278], 415: [0.9623, 0.9914, 0.3516], 416: [4.0649, 3.9506, 4.2576], 417: [2.7143, 2.9553], 418: [0.1837, 0.6142], 419: [4.1128, 4.0973], 420: [1.9366, 2.3681], 421: [3.9582, 4.2034], 422: [2.4353, 1.8927], 423: [0.2857, 0.1692], 424: [4.5854, 5.077], 425: [7.0936, 7.437, 6.0049, 6.6965], 426: [3.3593, 2.8616, 2.777, 2.7882, 3.2536], 427: [4.2388, 4.0078], 428: [4.0765, 3.7279, 3.7312, 3.9614, 3.9905], 429: [4.2273, 3.8049, 3.385], 430: [2.6831, 4.0738], 431: [2.5252, 2.7228, 3.8589, 2.5, 2.5083], 432: [3.5852, 2.6681], 433: [2.3228, 1.1918], 434: [3.1634, 2.6125, 0.4341], 435: [3.6497, 3.6374], 436: [4.1705, 3.7485, 3.9939], 437: [2.5753, 2.5724], 438: [2.8554, 3.6791], 439: [2.846, 3.1933], 440: [4.385, 4.3917], 441: [1.2522, 1.3093], 442: [3.4823, 3.3393], 443: [1.7858, 1.1025, 0.8158, 1.263, 1.1969, 0.7390000000000001, 0.7293, 0.7576], 444: [2.8568, 2.9509], 445: [1.6142, 3.9241], 446: [4.6888, 2.4494], 447: [2.6335, 3.4386], 448: [1.393, 1.585], 449: [4.2228, 4.3307], 450: [1.4731, 1.5494], 451: [1.9344, 0.8429, 2.0402], 452: [1.0381, 1.0289], 453: [1.3513, 1.3496], 454: [1.1806, 1.3398], 455: [2.5938, 2.4812], 456: [0.5751, 0.529], 457: [0.8205, 2.2672], 458: [1.8831, 1.9781], 459: [1.1426, 0.6031], 460: [2.2984, 2.3478], 461: [2.1658, 2.2647], 462: [1.2545, 1.1971], 463: [2.4381, 2.4325, 2.146], 464: [1.7769, 1.5919], 465: [0.5897, 0.5689], 466: [1.0376, 1.1163], 467: [2.2126, 2.4741, 2.2525], 468: [3.215, 3.6325], 469: [2.5498, 2.1806, 2.1623], 470: [3.3254, 2.721], 471: [2.7807, 2.2831], 472: [0.695, 0.6909], 473: [0.6542, 2.9375], 474: [3.2712, 3.3688], 475: [2.6562, 2.0179], 476: [2.4164, 2.8310000000000004], 477: [1.3914, 1.4212], 478: [2.8419, 0.8934], 479: [3.8433, 3.6896], 480: [2.7834, 2.7856, 0.655], 481: [2.6421, 2.9634], 482: [1.5496, 1.3066], 483: [1.4723, 1.3437, 1.6685], 484: [2.8302, 1.634], 485: [2.1065, 0.4413], 486: [2.9076, 2.1989], 487: [2.3465, 2.3211, 2.3391], 488: [3.0015, 2.9922], 489: [2.7848, 2.6886], 490: [2.0883, 2.0691], 491: [0.8318, 0.4262], 492: [0.1313, 0.0988, 0.6906], 493: [2.3857, 2.5915, 1.7808, 1.6737, 2.3008, 1.1169, 0.1632, 1.8636, 2.3477, 1.8151, 1.5887], 494: [3.7576, 3.7125], 495: [4.5405, 3.165], 496: [3.1207, 2.9066, 2.9134, 3.012], 497: [2.643, 2.6807, 2.6781], 498: [2.6626, 2.6686], 499: [2.8861, 1.83], 500: [2.6045, 0.1827], 501: [6.6073, 6.7529], 502: [6.3746, 6.7278], 503: [6.6394, 6.6592], 504: [7.4218, 7.4208], 505: [6.8664, 6.9792], 506: [6.0973, 5.9041, 5.8596], 507: [7.196000000000001, 6.9719], 508: [6.7782, 6.9302, 6.9172, 6.8938], 509: [6.8411, 6.8304], 510: [7.1819, 7.2354], 511: [5.9619, 6.2166], 512: [7.1474, 7.1316], 513: [6.236000000000001, 7.1318], 514: [6.6202, 6.1381], 515: [5.1370000000000005, 5.221], 516: [4.4954, 4.4963], 517: [2.7202, 2.6977, 2.8068, 2.7495], 518: [3.3463, 2.5727], 519: [2.8021, 2.681], 520: [2.7568, 2.7479], 521: [1.2891, 1.2836], 522: [4.968999999999999, 4.9507], 523: [5.7209, 5.8432], 524: [3.1215, 0.8698], 525: [6.9501, 6.8539, 6.9088], 526: [6.6758, 6.93], 527: [2.5398, 2.3848, 2.6312], 528: [2.1419, 3.8045], 529: [5.8453, 6.2943, 6.4424], 530: [3.2277, 3.2274], 531: [5.9351, 5.8168], 532: [5.8647, 6.7792], 533: [2.0752, 1.2626], 534: [3.0496, 2.8960000000000004], 535: [4.1848, 4.3481], 536: [1.3402, 2.6836], 537: [3.301, 3.3562, 3.3139], 538: [4.8782, 4.8644], 539: [4.8918, 4.916, 4.9347], 540: [4.7521, 4.6802], 541: [1.7973, 1.7597], 542: [4.1472, 4.3123, 4.7675], 543: [7.1932, 7.3216], 544: [6.3203, 6.2605], 545: [6.813, 6.6145], 546: [7.285, 6.989, 5.122999999999999, 6.1915], 547: [5.7264, 4.7619], 548: [4.2826, 5.2125], 549: [6.46, 6.9132, 6.8260000000000005], 550: [4.2567, 4.2407], 551: [3.0723, 3.1777, 3.0479], 552: [4.686, 3.9619], 553: [3.5323, 3.4902], 554: [3.4251, 3.4262], 555: [3.9916, 2.9408], 556: [1.2368, 0.8573], 557: [5.0416, 4.0312], 558: [0.5551, 0.7085], 559: [1.5374, 1.3625], 560: [0.3289, 0.1343], 561: [2.1322, 2.0340000000000003], 562: [0.4259, 0.8994], 563: [4.2554, 5.5624, 5.1433], 564: [1.4378, 0.8976, 0.5419], 565: [4.753, 4.5941], 566: [0.2136, 1.3181], 567: [2.3633, 0.4374, 2.3161], 568: [1.9593, 1.6283], 569: [3.2655, 3.164], 570: [2.2802, 1.5792], 571: [1.4349, 0.3911], 572: [0.2618, 1.1182, 0.2033, 0.2896], 573: [3.0329, 3.2421], 574: [1.1831, 1.2707], 575: [0.929, 0.9294], 576: [4.0656, 4.1012], 577: [1.1362, 1.3532], 578: [1.2025, 1.5714], 579: [2.7376, 2.5899], 580: [0.8916, 0.7296], 581: [0.1826, 3.5644], 582: [2.4543, 2.454], 583: [1.9771, 2.0844], 584: [1.0258, 1.2334], 585: [0.0935, 1.5404], 586: [1.395, 0.7775], 587: [1.7878, 1.8302], 588: [1.4511, 1.4453, 1.4819], 589: [2.4214, 2.293], 590: [1.0623, 1.6316, 0.2509], 591: [0.8428, 0.7518], 592: [0.3976, 2.2305], 593: [2.3659, 2.4698, 1.4742], 594: [1.4005, 2.4303], 595: [0.3062, 0.6382, 0.4076], 596: [0.1738, 0.1964], 597: [1.283, 0.8213], 598: [1.7399, 2.0113], 599: [0.4051, 0.9821, 1.0017, 0.7639, 0.7752, 1.1862], 600: [2.6859, 2.6784, 2.6971, 2.7172, 2.6864, 2.6821, 2.6996, 2.6934, 2.7125, 2.6933, 2.6722, 2.6891, 2.6765, 2.6901, 2.6762, 2.6921], 601: [2.069, 2.4419], 602: [1.6813, 1.4935], 603: [1.1957, 0.4485], 604: [2.0625, 1.952], 605: [0.0948, 0.8676, 1.0875], 606: [1.3117, 1.3430000000000002], 607: [1.8928, 1.959], 608: [1.8609, 2.465], 609: [2.3888, 0.7464], 610: [1.8538, 1.8549], 611: [0.6552, 1.4916], 612: [0.6681, 0.1602], 613: [0.4702, 0.5812], 614: [1.4076, 2.0432], 615: [1.3526, 0.3929, 0.9224], 616: [1.1886, 0.4117], 617: [0.3956, 0.922], 618: [1.0415, 0.9199], 619: [1.2793, 1.3687], 620: [0.2092, 0.9221, 1.1794, 0.2519], 621: [2.1222, 2.161], 622: [0.6494, 0.5774], 623: [0.1988, 1.5899], 624: [1.3714, 1.4003], 625: [1.7699, 1.5114], 626: [0.4129, 1.3839, 1.6321, 0.72, 1.5262, 1.0064], 627: [0.3425, 0.2391, 0.2967], 628: [0.5169, 0.8448], 629: [2.0459, 1.6782], 630: [0.1878, 1.4286], 631: [0.3537, 0.3473], 632: [2.1333, 2.2114], 633: [1.9183, 1.9138], 634: [0.6284, 1.1627, 1.4632], 635: [2.1533, 2.2264], 636: [1.5227, 1.4291], 637: [1.5382, 1.7402], 638: [0.934, 0.1242], 639: [1.4401, 1.4774], 640: [0.7829999999999999, 0.361], 641: [0.4542, 0.8209, 0.4654, 0.4657, 2.0492, 1.6893, 0.5348], 642: [5.0392, 4.8933], 643: [2.1388, 5.3373, 5.4449], 644: [2.5122, 2.5352, 2.5052, 2.476, 2.5287], 645: [2.6512, 2.6562], 646: [2.4464, 2.4473], 647: [1.9905, 1.9741], 648: [4.7227, 4.6902], 649: [4.6002, 4.3099, 4.6308], 650: [2.0580000000000003, 2.0969], 651: [1.0581, 1.0603], 652: [0.2359, 1.0662], 653: [3.7621, 3.6174], 654: [4.7085, 4.7129, 4.2169], 655: [4.7672, 4.8347], 656: [0.7065, 0.7435], 657: [2.0421, 1.6219], 658: [1.9564, 1.3776, 1.9714], 659: [2.3461, 3.6154], 660: [1.2742, 2.1626], 661: [2.3608, 2.5989, 2.1817], 662: [1.9268, 2.6235], 663: [0.9854, 2.8949], 664: [4.8961, 4.9557], 665: [4.5703, 3.5421], 666: [4.1055, 4.5118], 667: [1.9121, 1.8559], 668: [0.1679, 0.3155], 669: [2.6103, 2.8952], 670: [2.5246, 2.5838], 671: [1.4364, 1.4319], 672: [3.172, 3.8064], 673: [0.3912, 0.4649], 674: [1.3902, 1.421], 675: [3.0627, 3.0161], 676: [0.6855, 1.1861], 677: [1.4172, 1.1512], 678: [0.3984, 0.8271, 0.8284], 679: [0.7436, 1.4889], 680: [3.3839, 3.414], 681: [0.9152, 1.0939], 682: [1.0463, 1.2002], 683: [1.1642, 1.495], 684: [0.6098, 0.6229, 0.9169], 685: [1.1428, 0.7607, 0.8895], 686: [1.6514, 1.3491, 1.9689, 1.988, 1.6727], 687: [0.9403, 0.6228], 688: [3.7766, 3.7326, 3.8349, 3.8767], 689: [0.3078, 0.2945], 690: [1.0551, 0.7431, 0.7884, 0.9481], 691: [1.1676, 1.1571], 692: [0.5432, 1.2859], 693: [0.318, 0.6677], 694: [0.8234, 0.2777], 695: [2.9473, 2.9982], 696: [0.335, 0.5069, 0.3568], 697: [2.2065, 0.5529999999999999], 698: [1.9843, 1.8006], 699: [1.216, 0.7486, 1.3497], 700: [1.2365, 0.9151], 701: [0.4437, 0.3992], 702: [0.9888, 0.7001, 1.6158], 703: [1.1149, 1.0942], 704: [1.0333, 1.1223], 705: [0.4911, 0.1665, 1.071], 706: [0.8225, 0.9353], 707: [0.1978, 0.1559], 708: [0.9904, 0.4294, 0.3509], 709: [0.315, 0.2396], 710: [0.4476, 0.5486], 711: [1.9488, 1.2122], 712: [0.3759, 0.8489], 713: [1.5506, 1.029], 714: [1.0391, 0.1448], 715: [0.3984, 0.4024], 716: [0.4361, 0.4331, 0.4437], 717: [0.555, 0.4893, 0.9903], 718: [1.5724, 1.5578], 719: [1.0692, 3.0035, 3.0042], 720: [0.8264, 0.2681], 721: [1.6067, 2.8], 722: [2.0518, 2.2892], 723: [1.6469, 2.4767], 724: [2.2714, 2.1516], 725: [1.7415, 1.7537], 726: [2.5813, 2.5847], 727: [1.3786, 1.8929], 728: [0.4213, 0.7967], 729: [3.7921, 3.3478], 730: [0.4662, 1.7634], 731: [3.7903, 3.8006], 732: [0.2822, 0.2706], 733: [0.7467, 1.0812], 734: [0.1768, 0.1755], 735: [0.5173, 0.3168], 736: [1.1884, 1.1857], 737: [1.4666, 2.0035, 1.4728, 2.0867, 1.9779, 1.9774, 1.9771], 738: [0.2385, 0.2412], 739: [0.7440000000000001, 0.3069], 740: [2.3551, 2.3835, 2.3881, 2.2717, 2.3743, 2.3976, 2.3915, 2.4064, 2.4014, 2.3695, 2.4647, 2.3805, 2.3815, 2.4315, 2.3778, 2.3916, 2.3698, 2.3879], 741: [2.3116, 0.6203, 0.4577], 742: [0.5526, 0.5356], 743: [0.2559, 0.2364], 744: [3.8625, 3.6815], 745: [3.0622, 3.4691, 1.7261], 746: [3.0917, 3.2641, 3.3445], 747: [2.3475, 2.4091], 748: [1.4302, 2.0078], 749: [2.257, 1.2863, 1.4041], 750: [1.1361, 1.1342], 751: [1.7909, 2.1544], 752: [2.355, 2.385, 2.3749, 2.2929, 2.3831, 2.4003, 2.3518, 2.4094, 2.3978], 753: [2.4138, 2.5056, 2.2696, 2.2809], 754: [0.7103, 0.7022], 755: [6.2107, 6.1887], 756: [0.2108, 0.2221]}\n",
      "{0: 6.2445, 1: 4.0020750000000005, 2: 6.7893, 3: 5.0777, 4: 6.00335, 5: 2.36555, 6: 4.672766666666667, 7: 5.53055, 8: 4.8354, 9: 3.25405, 10: 6.411880000000001, 11: 3.5891, 12: 5.05115, 13: 5.70188, 14: 2.7786333333333335, 15: 2.9553999999999996, 16: 4.9846, 17: 4.9155, 18: 3.8263999999999996, 19: 5.359, 20: 2.61825, 21: 3.3309499999999996, 22: 5.59405, 23: 2.7777, 24: 5.2833000000000006, 25: 6.2836, 26: 3.75395, 27: 2.54745, 28: 4.8985, 29: 2.00855, 30: 2.8523, 31: 5.348675, 32: 5.035525, 33: 2.3148666666666666, 34: 1.823, 35: 2.899, 36: 0.49134999999999995, 37: 1.66445, 38: 4.53425, 39: 3.7601500000000003, 40: 4.7052, 41: 2.2022, 42: 2.0069500000000002, 43: 2.7286, 44: 2.977, 45: 2.04405, 46: 3.6089, 47: 2.8554500000000003, 48: 3.2493999999999996, 49: 1.109, 50: 3.37685, 51: 4.90185, 52: 1.85115, 53: 4.384885714285714, 54: 4.3956, 55: 4.73215, 56: 5.35075, 57: 0.15289999999999998, 58: 4.850980000000001, 59: 4.95528, 60: 7.3347999999999995, 61: 3.72675, 62: 3.29665, 63: 2.74845, 64: 2.9015000000000004, 65: 0.3732333333333333, 66: 0.8702333333333333, 67: 2.022133333333333, 68: 2.6917999999999997, 69: 3.4067499999999997, 70: 1.4149, 71: 1.5684, 72: 3.2108999999999996, 73: 0.8819999999999999, 74: 4.82135, 75: 3.6238, 76: 5.8294, 77: 0.5166, 78: 3.16655, 79: 2.9665, 80: 7.3210999999999995, 81: 1.07125, 82: 3.7386, 83: 3.0679, 84: 1.9359, 85: 0.8048, 86: 2.1607, 87: 3.3564, 88: 1.264, 89: 3.7876, 90: 3.68175, 91: 4.23445, 92: 3.5066, 93: 5.20105, 94: 5.26365, 95: 6.28205, 96: 4.18415, 97: 2.4763, 98: 2.28525, 99: 2.60005, 100: 2.98485, 101: 3.1361000000000003, 102: 4.66695, 103: 7.132133333333333, 104: 4.3515999999999995, 105: 0.5524, 106: 2.0645, 107: 6.47924, 108: 1.3908999999999998, 109: 5.9811, 110: 5.99865, 111: 6.7859, 112: 1.8874, 113: 3.987111111111111, 114: 6.2567, 115: 2.9055, 116: 5.1181, 117: 5.6549, 118: 1.1807, 119: 2.5929, 120: 6.0207999999999995, 121: 4.557766666666667, 122: 2.1746, 123: 2.9347999999999996, 124: 1.0767, 125: 2.17128, 126: 2.35348, 127: 6.535814285714286, 128: 2.094866666666667, 129: 4.318766666666666, 130: 1.1288, 131: 1.8804181818181818, 132: 4.12955, 133: 3.9981, 134: 4.7653, 135: 5.12545, 136: 4.4971, 137: 5.074999999999999, 138: 2.9902499999999996, 139: 1.991575, 140: 4.704225, 141: 1.8348666666666666, 142: 2.592233333333333, 143: 4.3387, 144: 3.5901, 145: 1.2963999999999998, 146: 1.9453500000000001, 147: 3.95145, 148: 3.40885, 149: 2.7973, 150: 1.02475, 151: 3.79495, 152: 1.14826, 153: 2.972, 154: 4.12815, 155: 4.40435, 156: 1.6425, 157: 0.63595, 158: 0.31395, 159: 0.3987, 160: 2.18135, 161: 3.78625, 162: 6.5341, 163: 1.9204666666666668, 164: 3.202, 165: 3.7691, 166: 3.5649500000000005, 167: 5.467666666666667, 168: 3.0769, 169: 3.02855, 170: 2.5031, 171: 2.391, 172: 1.2598999999999998, 173: 1.343925, 174: 1.6942799999999998, 175: 1.0493000000000001, 176: 1.3889, 177: 3.95615, 178: 0.9758000000000001, 179: 0.8368, 180: 1.5308000000000002, 181: 0.556, 182: 2.10815, 183: 2.4771, 184: 2.22785, 185: 3.5116, 186: 2.00365, 187: 2.5265000000000004, 188: 2.1111999999999997, 189: 1.4047999999999998, 190: 1.441, 191: 3.1627375000000004, 192: 4.5152833333333335, 193: 4.4247499999999995, 194: 4.266039999999999, 195: 4.960516666666667, 196: 5.321599999999999, 197: 4.596237037037038, 198: 4.912999999999999, 199: 5.3584499999999995, 200: 4.989599999999999, 201: 5.059666666666666, 202: 5.556850000000001, 203: 4.5129, 204: 4.99815, 205: 3.5659000000000005, 206: 5.340299999999999, 207: 4.6808, 208: 4.8296, 209: 4.0529, 210: 4.287599999999999, 211: 5.51295, 212: 3.929733333333333, 213: 3.6500500000000002, 214: 2.8800499999999998, 215: 2.6790000000000003, 216: 2.6156666666666664, 217: 2.3918999999999997, 218: 1.2429999999999999, 219: 2.6696, 220: 1.5318333333333332, 221: 1.7702666666666669, 222: 1.5185, 223: 3.1172500000000003, 224: 2.0187666666666666, 225: 1.13855, 226: 0.6208666666666666, 227: 2.3766999999999996, 228: 3.0271999999999997, 229: 1.9956999999999998, 230: 1.1509500000000001, 231: 2.82005, 232: 0.8098, 233: 0.6365999999999999, 234: 2.3923, 235: 0.581, 236: 2.1024333333333334, 237: 3.6026, 238: 1.7545000000000002, 239: 0.9675000000000001, 240: 2.3891999999999998, 241: 1.6637499999999998, 242: 1.8126, 243: 0.9509333333333334, 244: 0.5670666666666667, 245: 1.10745, 246: 3.2646333333333337, 247: 1.9252666666666667, 248: 2.224766666666667, 249: 0.88525, 250: 2.59895, 251: 1.79215, 252: 1.1021999999999998, 253: 1.39015, 254: 0.73095, 255: 1.22335, 256: 0.93925, 257: 2.64025, 258: 3.5997500000000002, 259: 4.62565, 260: 4.36055, 261: 4.1066, 262: 2.42305, 263: 2.6121499999999997, 264: 1.8756499999999998, 265: 3.381866666666667, 266: 1.7587000000000002, 267: 2.01015, 268: 1.0336, 269: 3.00195, 270: 2.9666, 271: 2.5269, 272: 2.80055, 273: 1.7624833333333332, 274: 0.9447, 275: 2.2621, 276: 4.0687999999999995, 277: 3.91975, 278: 2.8916, 279: 1.3998, 280: 1.0121, 281: 2.13025, 282: 1.0914000000000001, 283: 3.37915, 284: 3.84965, 285: 3.2998666666666665, 286: 0.9954, 287: 4.01725, 288: 0.7838, 289: 4.2447, 290: 5.3344000000000005, 291: 1.45335, 292: 1.2014999999999998, 293: 2.0766999999999998, 294: 2.3382, 295: 0.7034333333333334, 296: 3.7676999999999996, 297: 3.0096999999999996, 298: 4.7395, 299: 2.8853999999999997, 300: 0.42560000000000003, 301: 2.58515, 302: 3.9073, 303: 4.033633333333333, 304: 2.75955, 305: 4.6966, 306: 4.28425, 307: 3.9305, 308: 4.118166666666667, 309: 3.4975666666666663, 310: 3.336866666666667, 311: 1.60975, 312: 2.8340666666666667, 313: 2.1593714285714283, 314: 2.0013, 315: 2.163, 316: 1.22335, 317: 2.1891499999999997, 318: 1.6269, 319: 3.0425, 320: 1.5576499999999998, 321: 2.0444, 322: 1.5117500000000001, 323: 2.01885, 324: 0.47675, 325: 2.4369500000000004, 326: 1.6936499999999999, 327: 1.7854, 328: 3.5090500000000002, 329: 1.24495, 330: 2.4029499999999997, 331: 1.5836999999999999, 332: 2.45015, 333: 0.9411499999999999, 334: 0.62995, 335: 1.4200499999999998, 336: 1.5004, 337: 1.4489, 338: 2.2747333333333333, 339: 2.375, 340: 3.36745, 341: 2.9924999999999997, 342: 1.4724000000000002, 343: 3.32655, 344: 2.0797, 345: 2.96335, 346: 0.44505, 347: 1.0612, 348: 1.08365, 349: 2.2076, 350: 1.82105, 351: 0.27275, 352: 0.9732666666666666, 353: 2.44975, 354: 0.7868999999999999, 355: 1.3782999999999999, 356: 1.9002, 357: 1.4736333333333331, 358: 0.8482500000000001, 359: 2.30315, 360: 2.7441, 361: 1.67565, 362: 2.6638, 363: 1.58865, 364: 3.23115, 365: 3.2782, 366: 2.50195, 367: 3.1999, 368: 1.7614, 369: 1.5842, 370: 3.9869, 371: 4.4506, 372: 2.522466666666667, 373: 2.9200999999999997, 374: 1.43005, 375: 2.5858999999999996, 376: 4.2543500000000005, 377: 1.5436, 378: 1.5719500000000002, 379: 3.6585, 380: 1.7062, 381: 2.8182, 382: 1.3163500000000001, 383: 1.4526, 384: 0.98455, 385: 3.5849, 386: 1.72475, 387: 2.4013666666666666, 388: 3.2384999999999997, 389: 2.4418333333333333, 390: 3.3496, 391: 2.07265, 392: 3.37805, 393: 4.53325, 394: 1.28465, 395: 1.34535, 396: 3.6485000000000003, 397: 3.6453500000000005, 398: 1.65845, 399: 2.0256, 400: 1.9989, 401: 3.63415, 402: 3.00345, 403: 2.9034, 404: 2.6698333333333335, 405: 0.7440500000000001, 406: 0.1611, 407: 3.346633333333333, 408: 3.7021000000000006, 409: 3.8352000000000004, 410: 2.96335, 411: 0.42389999999999994, 412: 3.6563, 413: 4.0185, 414: 3.9599, 415: 0.7684333333333333, 416: 4.091033333333333, 417: 2.8348, 418: 0.39894999999999997, 419: 4.10505, 420: 2.15235, 421: 4.0808, 422: 2.1639999999999997, 423: 0.22744999999999999, 424: 4.8312, 425: 6.808, 426: 3.0079400000000005, 427: 4.1233, 428: 3.8975, 429: 3.805733333333333, 430: 3.37845, 431: 2.8230399999999998, 432: 3.1266499999999997, 433: 1.7572999999999999, 434: 2.07, 435: 3.6435500000000003, 436: 3.970966666666667, 437: 2.57385, 438: 3.2672499999999998, 439: 3.01965, 440: 4.38835, 441: 1.2807499999999998, 442: 3.4108, 443: 1.0487375, 444: 2.90385, 445: 2.7691500000000002, 446: 3.5690999999999997, 447: 3.0360500000000004, 448: 1.4889999999999999, 449: 4.27675, 450: 1.51125, 451: 1.6058333333333332, 452: 1.0335, 453: 1.35045, 454: 1.2602000000000002, 455: 2.5374999999999996, 456: 0.5520499999999999, 457: 1.54385, 458: 1.9306, 459: 0.87285, 460: 2.3231, 461: 2.21525, 462: 1.2258, 463: 2.3388666666666666, 464: 1.6844000000000001, 465: 0.5792999999999999, 466: 1.07695, 467: 2.3130666666666664, 468: 3.42375, 469: 2.2975666666666665, 470: 3.0232, 471: 2.5319000000000003, 472: 0.69295, 473: 1.79585, 474: 3.32, 475: 2.33705, 476: 2.6237000000000004, 477: 1.4062999999999999, 478: 1.8676499999999998, 479: 3.76645, 480: 2.074666666666667, 481: 2.80275, 482: 1.4281000000000001, 483: 1.4948333333333332, 484: 2.2321, 485: 1.2739, 486: 2.5532500000000002, 487: 2.335566666666667, 488: 2.9968500000000002, 489: 2.7367, 490: 2.0787, 491: 0.629, 492: 0.3069, 493: 1.7843363636363636, 494: 3.73505, 495: 3.85275, 496: 2.988175, 497: 2.6672666666666665, 498: 2.6656, 499: 2.35805, 500: 1.3936, 501: 6.6801, 502: 6.5512, 503: 6.6493, 504: 7.4213000000000005, 505: 6.9228, 506: 5.953666666666667, 507: 7.08395, 508: 6.87985, 509: 6.83575, 510: 7.2086500000000004, 511: 6.08925, 512: 7.1395, 513: 6.6839, 514: 6.379149999999999, 515: 5.179, 516: 4.49585, 517: 2.74355, 518: 2.9595000000000002, 519: 2.74155, 520: 2.75235, 521: 1.28635, 522: 4.959849999999999, 523: 5.78205, 524: 1.9956500000000001, 525: 6.904266666666667, 526: 6.802899999999999, 527: 2.5185999999999997, 528: 2.9732000000000003, 529: 6.194, 530: 3.22755, 531: 5.87595, 532: 6.32195, 533: 1.6689, 534: 2.9728000000000003, 535: 4.26645, 536: 2.0119000000000002, 537: 3.3237, 538: 4.8713, 539: 4.914166666666667, 540: 4.716150000000001, 541: 1.7785, 542: 4.409, 543: 7.2574000000000005, 544: 6.2904, 545: 6.713749999999999, 546: 6.397124999999999, 547: 5.244149999999999, 548: 4.74755, 549: 6.733066666666667, 550: 4.2487, 551: 3.0993, 552: 4.32395, 553: 3.5112500000000004, 554: 3.42565, 555: 3.4661999999999997, 556: 1.04705, 557: 4.5364, 558: 0.6318, 559: 1.44995, 560: 0.23160000000000003, 561: 2.0831, 562: 0.66265, 563: 4.987033333333334, 564: 0.9591, 565: 4.6735500000000005, 566: 0.76585, 567: 1.7055999999999998, 568: 1.7938, 569: 3.21475, 570: 1.9297, 571: 0.913, 572: 0.46822500000000006, 573: 3.1375, 574: 1.2269, 575: 0.9292, 576: 4.0834, 577: 1.2447, 578: 1.38695, 579: 2.6637500000000003, 580: 0.8106, 581: 1.8735, 582: 2.4541500000000003, 583: 2.0307500000000003, 584: 1.1296, 585: 0.81695, 586: 1.08625, 587: 1.8090000000000002, 588: 1.4594333333333331, 589: 2.3572, 590: 0.9816000000000001, 591: 0.7973, 592: 1.3140500000000002, 593: 2.1033, 594: 1.9154, 595: 0.4506666666666667, 596: 0.1851, 597: 1.05215, 598: 1.8756, 599: 0.8523666666666667, 600: 2.6901312500000003, 601: 2.2554499999999997, 602: 1.5874000000000001, 603: 0.8221, 604: 2.00725, 605: 0.6833, 606: 1.32735, 607: 1.9259, 608: 2.16295, 609: 1.5675999999999999, 610: 1.85435, 611: 1.0734, 612: 0.41415, 613: 0.5257000000000001, 614: 1.7254, 615: 0.8893, 616: 0.80015, 617: 0.6588, 618: 0.9807000000000001, 619: 1.324, 620: 0.6406499999999999, 621: 2.1416, 622: 0.6134, 623: 0.8943500000000001, 624: 1.38585, 625: 1.64065, 626: 1.1135833333333334, 627: 0.2927666666666667, 628: 0.68085, 629: 1.86205, 630: 0.8082, 631: 0.35050000000000003, 632: 2.17235, 633: 1.9160499999999998, 634: 1.0847666666666667, 635: 2.18985, 636: 1.4759, 637: 1.6392, 638: 0.5291, 639: 1.45875, 640: 0.572, 641: 0.9256428571428571, 642: 4.9662500000000005, 643: 4.3069999999999995, 644: 2.5114600000000005, 645: 2.6536999999999997, 646: 2.44685, 647: 1.9823, 648: 4.70645, 649: 4.513633333333334, 650: 2.0774500000000002, 651: 1.0592000000000001, 652: 0.65105, 653: 3.68975, 654: 4.5461, 655: 4.80095, 656: 0.7250000000000001, 657: 1.8319999999999999, 658: 1.7684666666666666, 659: 2.98075, 660: 1.7184, 661: 2.3804666666666665, 662: 2.27515, 663: 1.94015, 664: 4.9259, 665: 4.0562, 666: 4.30865, 667: 1.884, 668: 0.2417, 669: 2.75275, 670: 2.5542, 671: 1.4341499999999998, 672: 3.4892000000000003, 673: 0.42805, 674: 1.4056000000000002, 675: 3.0393999999999997, 676: 0.9358, 677: 1.2842, 678: 0.6846333333333332, 679: 1.11625, 680: 3.39895, 681: 1.00455, 682: 1.12325, 683: 1.3296000000000001, 684: 0.7165333333333334, 685: 0.931, 686: 1.72602, 687: 0.78155, 688: 3.8052, 689: 0.30115000000000003, 690: 0.883675, 691: 1.16235, 692: 0.91455, 693: 0.49285, 694: 0.55055, 695: 2.97275, 696: 0.3995666666666667, 697: 1.37975, 698: 1.89245, 699: 1.1047666666666667, 700: 1.0758, 701: 0.42145, 702: 1.1015666666666666, 703: 1.1045500000000001, 704: 1.0778, 705: 0.5761999999999999, 706: 0.8789, 707: 0.17685, 708: 0.5902333333333333, 709: 0.2773, 710: 0.4981, 711: 1.5805, 712: 0.6124, 713: 1.2898, 714: 0.59195, 715: 0.4004, 716: 0.4376333333333333, 717: 0.6782, 718: 1.5651000000000002, 719: 2.3589666666666664, 720: 0.54725, 721: 2.20335, 722: 2.1705, 723: 2.0618, 724: 2.2115, 725: 1.7476, 726: 2.583, 727: 1.63575, 728: 0.609, 729: 3.56995, 730: 1.1148, 731: 3.7954499999999998, 732: 0.2764, 733: 0.91395, 734: 0.17615, 735: 0.41705000000000003, 736: 1.18705, 737: 1.8517142857142856, 738: 0.23985, 739: 0.5254500000000001, 740: 2.384688888888889, 741: 1.1298666666666666, 742: 0.5441, 743: 0.24615, 744: 3.7720000000000002, 745: 2.752466666666667, 746: 3.2334333333333336, 747: 2.3783000000000003, 748: 1.7189999999999999, 749: 1.6491333333333333, 750: 1.13515, 751: 1.9726499999999998, 752: 2.3722444444444446, 753: 2.3674749999999998, 754: 0.70625, 755: 6.1997, 756: 0.21644999999999998}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with exactly one unique value and the unique value:\n",
      "D97: 0\n",
      "D121: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "      <th>D6</th>\n",
       "      <th>D7</th>\n",
       "      <th>D8</th>\n",
       "      <th>D9</th>\n",
       "      <th>D10</th>\n",
       "      <th>...</th>\n",
       "      <th>D123</th>\n",
       "      <th>D124</th>\n",
       "      <th>D125</th>\n",
       "      <th>D126</th>\n",
       "      <th>D127</th>\n",
       "      <th>D128</th>\n",
       "      <th>D129</th>\n",
       "      <th>D130</th>\n",
       "      <th>D131</th>\n",
       "      <th>D132</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>39</td>\n",
       "      <td>31</td>\n",
       "      <td>15.6667</td>\n",
       "      <td>10.22220</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>87</td>\n",
       "      <td>75</td>\n",
       "      <td>67.6667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>229</td>\n",
       "      <td>217</td>\n",
       "      <td>78.5000</td>\n",
       "      <td>88.666700</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>56</td>\n",
       "      <td>47</td>\n",
       "      <td>17.6250</td>\n",
       "      <td>12.43750</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>93</td>\n",
       "      <td>84</td>\n",
       "      <td>77.6250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>229</td>\n",
       "      <td>214</td>\n",
       "      <td>81.1250</td>\n",
       "      <td>90.921900</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>46</td>\n",
       "      <td>43</td>\n",
       "      <td>13.6000</td>\n",
       "      <td>12.96000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>87</td>\n",
       "      <td>86</td>\n",
       "      <td>47.6000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>229</td>\n",
       "      <td>217</td>\n",
       "      <td>141.4000</td>\n",
       "      <td>103.520000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>37</td>\n",
       "      <td>20</td>\n",
       "      <td>29.5714</td>\n",
       "      <td>7.34694</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>95</td>\n",
       "      <td>91</td>\n",
       "      <td>64.2857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64</td>\n",
       "      <td>229</td>\n",
       "      <td>165</td>\n",
       "      <td>134.1430</td>\n",
       "      <td>80.163300</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>11</td>\n",
       "      <td>12.0000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>87</td>\n",
       "      <td>84</td>\n",
       "      <td>62.5000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>229</td>\n",
       "      <td>217</td>\n",
       "      <td>119.5000</td>\n",
       "      <td>107.500000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9634</th>\n",
       "      <td>5</td>\n",
       "      <td>38</td>\n",
       "      <td>33</td>\n",
       "      <td>13.8333</td>\n",
       "      <td>9.75000</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>87</td>\n",
       "      <td>79</td>\n",
       "      <td>75.7500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>225</td>\n",
       "      <td>213</td>\n",
       "      <td>64.0833</td>\n",
       "      <td>60.791700</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9635</th>\n",
       "      <td>8</td>\n",
       "      <td>37</td>\n",
       "      <td>29</td>\n",
       "      <td>21.8182</td>\n",
       "      <td>12.56200</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>87</td>\n",
       "      <td>83</td>\n",
       "      <td>51.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.11066</td>\n",
       "      <td>0.383757</td>\n",
       "      <td>0.627966</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>229</td>\n",
       "      <td>217</td>\n",
       "      <td>130.3640</td>\n",
       "      <td>107.603000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9637</th>\n",
       "      <td>8</td>\n",
       "      <td>38</td>\n",
       "      <td>30</td>\n",
       "      <td>12.9333</td>\n",
       "      <td>6.57778</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>87</td>\n",
       "      <td>79</td>\n",
       "      <td>75.7333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>229</td>\n",
       "      <td>227</td>\n",
       "      <td>53.8000</td>\n",
       "      <td>69.546700</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9638</th>\n",
       "      <td>14</td>\n",
       "      <td>38</td>\n",
       "      <td>24</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>10.66670</td>\n",
       "      <td>38</td>\n",
       "      <td>8</td>\n",
       "      <td>78</td>\n",
       "      <td>70</td>\n",
       "      <td>31.3333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>225</td>\n",
       "      <td>227</td>\n",
       "      <td>2</td>\n",
       "      <td>225.6670</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9639</th>\n",
       "      <td>17</td>\n",
       "      <td>37</td>\n",
       "      <td>20</td>\n",
       "      <td>22.3333</td>\n",
       "      <td>7.11111</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>94</td>\n",
       "      <td>90</td>\n",
       "      <td>69.3333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00031</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64</td>\n",
       "      <td>229</td>\n",
       "      <td>165</td>\n",
       "      <td>117.6670</td>\n",
       "      <td>71.555600</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8426 rows × 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      D1  D2  D3       D4        D5  D6  D7  D8  D9      D10  ...     D123  \\\n",
       "0      8  39  31  15.6667  10.22220   8  12  87  75  67.6667  ...  0.00000   \n",
       "1      9  56  47  17.6250  12.43750   9   9  93  84  77.6250  ...  0.00000   \n",
       "2      3  46  43  13.6000  12.96000   3   1  87  86  47.6000  ...  0.00000   \n",
       "3     17  37  20  29.5714   7.34694  17   4  95  91  64.2857  ...  0.00000   \n",
       "4      8  19  11  12.0000   4.00000   8   3  87  84  62.5000  ...  0.00000   \n",
       "...   ..  ..  ..      ...       ...  ..  ..  ..  ..      ...  ...      ...   \n",
       "9634   5  38  33  13.8333   9.75000   8   8  87  79  75.7500  ...  0.00000   \n",
       "9635   8  37  29  21.8182  12.56200   8   4  87  83  51.0000  ...  2.11066   \n",
       "9637   8  38  30  12.9333   6.57778   8   8  87  79  75.7333  ...  0.00000   \n",
       "9638  14  38  24  30.0000  10.66670  38   8  78  70  31.3333  ...  0.00000   \n",
       "9639  17  37  20  22.3333   7.11111  17   4  94  90  69.3333  ...  0.00031   \n",
       "\n",
       "          D124      D125  D126  D127  D128  D129      D130        D131  D132  \n",
       "0     0.000000  0.000000   0.0    12   229   217   78.5000   88.666700    12  \n",
       "1     0.000000  0.000000   0.0    15   229   214   81.1250   90.921900    15  \n",
       "2     0.000000  0.000000   0.0    12   229   217  141.4000  103.520000    12  \n",
       "3     0.000000  0.000000   0.0    64   229   165  134.1430   80.163300    64  \n",
       "4     0.000000  0.000000   0.0    12   229   217  119.5000  107.500000    12  \n",
       "...        ...       ...   ...   ...   ...   ...       ...         ...   ...  \n",
       "9634  0.000000  0.000000   0.0    12   225   213   64.0833   60.791700    12  \n",
       "9635  0.383757  0.627966   0.0    12   229   217  130.3640  107.603000    12  \n",
       "9637  0.000000  0.000000   0.0     2   229   227   53.8000   69.546700    12  \n",
       "9638  0.000000  0.000000   0.0   225   227     2  225.6670    0.888889   225  \n",
       "9639  0.000034  0.000061   0.0    64   229   165  117.6670   71.555600    64  \n",
       "\n",
       "[8426 rows x 130 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = pd.read_csv(\"train_X.csv\")\n",
    "train_y = pd.read_csv(\"train_y.csv\")\n",
    "test_x = pd.read_csv(\"test_X.csv\")\n",
    "test_x1 = test_x.drop(['Material', 'Id'], axis=1)\n",
    "train_x1 = train_x.drop(['Material', 'Id'], axis=1)\n",
    "train_x1\n",
    "import pandas as pd\n",
    "\n",
    "# Group by all columns\n",
    "grouped = train_x1.groupby(train_x1.columns.tolist())\n",
    "\n",
    "# Initialize a dictionary to store indices of each group\n",
    "group_indices = {}\n",
    "\n",
    "# Iterate through groups and store indices\n",
    "group_number = 0\n",
    "for _, group in grouped:\n",
    "    if len(group) > 1:  # Only consider groups with more than one row\n",
    "        group_indices[group_number] = group.index.tolist()\n",
    "        group_number += 1\n",
    "\n",
    "# Print or use the group indices\n",
    "print(group_indices)\n",
    "# Assuming group_indices is your dictionary from test_x1\n",
    "# And train_y is a DataFrame or Series with the 'Egap' column\n",
    "\n",
    "# Initialize a dictionary to store Egap values for each group\n",
    "grouped_egap_values = {}\n",
    "\n",
    "for group_num, indices in group_indices.items():\n",
    "    # Extract corresponding Egap values for the group of indices\n",
    "    grouped_egap_values[group_num] = train_y.loc[indices, 'Egap'].tolist()\n",
    "\n",
    "# Print or use the grouped Egap values\n",
    "print(grouped_egap_values)\n",
    "# Initialize a dictionary to store the average Egap for each group\n",
    "average_egap_per_group = {}\n",
    "\n",
    "for group_num, egap_values in grouped_egap_values.items():\n",
    "    # Calculate the average Egap for the group\n",
    "    average_egap_per_group[group_num] = sum(egap_values) / len(egap_values) if egap_values else 0\n",
    "\n",
    "# Print or use the average Egap values per group\n",
    "print(average_egap_per_group)\n",
    "train_y1= train_y.copy()\n",
    "for group_num, indices in group_indices.items():\n",
    "    # Get the average Egap for this group\n",
    "    avg_egap = average_egap_per_group[group_num]\n",
    "\n",
    "    # Update the Egap values for the rows in this group\n",
    "    train_y1.loc[indices, 'Egap'] = avg_egap\n",
    "# Initialize an empty list to store the indices of rows to drop\n",
    "rows_to_drop = []\n",
    "\n",
    "# Iterate through the group_indices\n",
    "for indices in group_indices.values():\n",
    "    if len(indices) > 1:\n",
    "        # Add all indices except the first one to the rows_to_drop list\n",
    "        rows_to_drop.extend(indices[1:])\n",
    "\n",
    "# Drop these rows from train_x1 and train_y1\n",
    "train_x2 = train_x1.drop(rows_to_drop)\n",
    "train_y2 = train_y1.drop(rows_to_drop)\n",
    "train_y2 = train_y2.drop(['Id'], axis=1)\n",
    "train_x2\n",
    "unique_values_count = train_x2.nunique()\n",
    "unique_values_count\n",
    "\n",
    "# Filter columns where the number of unique values is exactly 1\n",
    "columns_with_single_unique_value = unique_values_count[unique_values_count == 1].index\n",
    "\n",
    "print(\"Columns with exactly one unique value and the unique value:\")\n",
    "for column in columns_with_single_unique_value:\n",
    "    # Find the unique value in the column\n",
    "    unique_value = train_x[column].unique()[0]\n",
    "    print(f\"{column}: {unique_value}\")\n",
    "\n",
    "train_x2 = train_x2.drop(columns_with_single_unique_value, axis=1)\n",
    "test_x1= test_x1.drop(columns_with_single_unique_value, axis=1)\n",
    "train_x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pITbi8WtGVn0"
   },
   "source": [
    "## All Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qbd24Fa-mC_N"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "\n",
    "models = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'Ridge': Ridge(),\n",
    "    'Lasso': Lasso(),\n",
    "    'ElasticNet': ElasticNet(),\n",
    "    'PolynomialRegression': make_pipeline(PolynomialFeatures(degree=2), LinearRegression()),\n",
    "    'MLPRegressor': MLPRegressor(max_iter=1000),\n",
    "    'SVR': SVR(),\n",
    "    'XGBoost': XGBRegressor(n_jobs=-1),\n",
    "    'RandomForest': RandomForestRegressor(n_jobs=-1),\n",
    "    'GradientBoosting': GradientBoostingRegressor(),\n",
    "    'LightGBM': LGBMRegressor(n_jobs=-1),\n",
    "    'DecisionTree': DecisionTreeRegressor(),\n",
    "    'KNN': KNeighborsRegressor()\n",
    "\n",
    "}\n",
    "\n",
    "# Cross-validation settings\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Custom scorer for MAE\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "# Evaluate each model\n",
    "for name, model in models.items():\n",
    "    cv_mae = cross_val_score(model, train_x2, train_y2, cv=kf, scoring=mae_scorer, n_jobs=-1)\n",
    "    cv_r2 = cross_val_score(model, train_x2, train_y2, cv=kf, scoring='r2', n_jobs=-1)\n",
    "    print(f\"{name}: Mean MAE: {-np.mean(cv_mae):.4f}, Mean R2 Score: {np.mean(cv_r2):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Byh5OMkbGPgi",
    "outputId": "c1f10a34-bfc4-4bb9-cc5a-a84998954b48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost: Mean MAE: 0.5700, Mean R2 Score: 0.7396\n",
      "RandomForest: Mean MAE: 0.5780, Mean R2 Score: 0.7310\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error, r2_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "models = {\n",
    "\n",
    "    'XGBoost': XGBRegressor(n_jobs=-1),\n",
    "    'RandomForest': RandomForestRegressor(n_jobs=-1)\n",
    "}\n",
    "\n",
    "# Cross-validation settings\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Custom scorer for MAE\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "# Evaluate each model\n",
    "for name, model in models.items():\n",
    "    cv_mae = cross_val_score(model, train_x2, train_y2, cv=kf, scoring=mae_scorer, n_jobs=-1)\n",
    "    cv_r2 = cross_val_score(model, train_x2, train_y2, cv=kf, scoring='r2', n_jobs=-1)\n",
    "    print(f\"{name}: Mean MAE: {-np.mean(cv_mae):.4f}, Mean R2 Score: {np.mean(cv_r2):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aHlxghtsbl85",
    "outputId": "6624ccf0-4b1e-4c14-aa16-aa7ae2a892e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost: Mean MAE: 0.5056, Mean R2 Score: 0.7825\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define the optimized parameters for XGBoost\n",
    "xgb_params = {\n",
    "\n",
    "    'subsample':  0.95,  #0.8\n",
    "    'n_estimators': int(4500),  # Convert to integer\n",
    "    'min_child_weight': 2.1,\n",
    "    'max_depth': int(7),  # Convert to integer\n",
    "    'learning_rate': 0.02,\n",
    "    'gamma': 0, #0\n",
    "    'colsample_bytree': 0.65, #0.8\n",
    "    'nthread': 4,\n",
    "    'scale_pos_weight' : 1,\n",
    "    'reg_alpha': 1e-05,\n",
    "    'objective':'reg:squarederror'\n",
    "}\n",
    "\n",
    "\n",
    "# Models to test\n",
    "models = {\n",
    "    'XGBoost': XGBRegressor(**xgb_params)\n",
    "}\n",
    "'''scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(top_100)'''\n",
    "\n",
    "# Cross-validation settings\n",
    "kf = KFold(n_splits=7, shuffle=True, random_state=77)\n",
    "\n",
    "# Custom scorer for MAE\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "# Evaluate each model using cross-validation\n",
    "for name, model in models.items():\n",
    "    cv_mae = cross_val_score(model, top_100_dropped, top_100y, cv=kf, scoring=mae_scorer, n_jobs=-1)\n",
    "    cv_r2 = cross_val_score(model, top_100_dropped, top_100y, cv=kf, scoring='r2', n_jobs=-1)\n",
    "    print(f\"{name}: Mean MAE: {-np.mean(cv_mae):.4f}, Mean R2 Score: {np.mean(cv_r2):.4f}\")\n",
    "\n",
    "# Train the model on the entire dataset\n",
    "model = XGBRegressor(**xgb_params)\n",
    "model.fit(top_100_dropped, top_100y)\n",
    "\n",
    "# Assuming test_x is your test dataset\n",
    "# Make sure test_x is prepared in the same way as your training data\n",
    "# Predict on the test set\n",
    "predicted_egap = model.predict(test_x1_dropped)\n",
    "\n",
    "# Prepare submission DataFrame\n",
    "# Replace 'Id' with the actual ID column of your test dataset\n",
    "submission_df = pd.DataFrame()\n",
    "submission_df['Id'] = test_x['Id']\n",
    "submission_df['Egap'] = predicted_egap\n",
    "\n",
    "# Save predictions to CSV\n",
    "filename = f\"XGBoost_predictions_{-np.mean(cv_mae):.4f}.csv\"\n",
    "submission_df.to_csv(filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x4 = pd.read_csv(\"train_x4.csv\")\n",
    "train_y4 = pd.read_csv(\"train_y4.csv\")\n",
    "test_x2 = pd.read_csv(\"test_x2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-7xAwf98qsp"
   },
   "source": [
    "## Tuning set by set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QUlKpG5JFaMa",
    "outputId": "426b4506-6197-4249-8508-0dc4f86b4d71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost: Mean MAE: 0.5113, Mean R2 Score: 0.7853\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define the optimized parameters for XGBoost\n",
    "xgb_params = {\n",
    "\n",
    "    'subsample':  0.9298530839485764,\n",
    "    'n_estimators': int(3900),  # Convert to integer\n",
    "    'min_child_weight': 5,\n",
    "    \n",
    "    'max_depth': int(7),  # Convert to integer\n",
    "    \n",
    "    'learning_rate': 0.022132607343053803,\n",
    "    \n",
    "    'gamma': 0.009553172407224229,\n",
    "    \n",
    "    'colsample_bytree': 0.276213091190313,\n",
    "    \n",
    "    'nthread': 4,\n",
    "    'scale_pos_weight' : 1,\n",
    "    'reg_alpha': 1,\n",
    "    'objective':'reg:squarederror'\n",
    "}\n",
    "\n",
    "# Models to test\n",
    "models = {\n",
    "    'XGBoost': XGBRegressor(**xgb_params)\n",
    "}\n",
    "'''scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(top_100)'''\n",
    "\n",
    "# Cross-validation settings\n",
    "kf = KFold(n_splits=7, shuffle=True, random_state=77)\n",
    "\n",
    "# Custom scorer for MAE\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "# Evaluate each model using cross-validation\n",
    "for name, model in models.items():\n",
    "    cv_mae = cross_val_score(model, train_x4, train_y4, cv=kf, scoring=mae_scorer, n_jobs=-1)\n",
    "    cv_r2 = cross_val_score(model, train_x4, train_y4, cv=kf, scoring='r2', n_jobs=-1)\n",
    "    print(f\"{name}: Mean MAE: {-np.mean(cv_mae):.4f}, Mean R2 Score: {np.mean(cv_r2):.4f}\")\n",
    "\n",
    "# Train the model on the entire dataset\n",
    "model = XGBRegressor(**xgb_params)\n",
    "model.fit(train_x4, train_y4)\n",
    "\n",
    "# Assuming test_x is your test dataset\n",
    "# Make sure test_x is prepared in the same way as your training data\n",
    "# Predict on the test set\n",
    "predicted_egap = model.predict(test_x2)\n",
    "\n",
    "# Prepare submission DataFrame\n",
    "# Replace 'Id' with the actual ID column of your test dataset\n",
    "submission_df = pd.DataFrame()\n",
    "submission_df['Id'] = test_x['Id']\n",
    "submission_df['Egap'] = predicted_egap\n",
    "\n",
    "# Save predictions to CSV\n",
    "filename = f\"XGBoost_predictions_{-np.mean(cv_mae):.4f}.csv\"\n",
    "submission_df.to_csv(filename, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d211tI_6O_0R"
   },
   "source": [
    "### Tuning max_depth and min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IKNeCfFdHoFz",
    "outputId": "f796ae6d-3e52-43e5-c810-51538e9b1619"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      "{'max_depth': 9, 'min_child_weight': 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "param_test1 = {\n",
    "    'max_depth': range(3, 10, 2),\n",
    "    'min_child_weight': range(1, 6, 2)\n",
    "}\n",
    "\n",
    "gsearch1 = GridSearchCV(\n",
    "    estimator=XGBRegressor(\n",
    "        learning_rate=0.02,\n",
    "        n_estimators=140,\n",
    "        max_depth=7,\n",
    "        min_child_weight=2.1,\n",
    "        gamma=0,\n",
    "        subsample=0.95,\n",
    "        colsample_bytree=0.65,\n",
    "        objective='reg:squarederror',\n",
    "        nthread=4,\n",
    "        scale_pos_weight=1,\n",
    "        seed=27\n",
    "    ),\n",
    "    param_grid=param_test1,\n",
    "    scoring='neg_mean_squared_error',  # Changed to a regression scoring metric\n",
    "    n_jobs=4,\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "gsearch1.fit(train_x4, train_y4)\n",
    "\n",
    "# Use cv_results_ for grid scores\n",
    "print(\"Best Parameters:\")\n",
    "print(gsearch1.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qjgoxzwFMosL",
    "outputId": "833e659a-4a60-4da8-b91b-919281813ef0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      "{'max_depth': 8}\n"
     ]
    }
   ],
   "source": [
    "param_test2 = {\n",
    "    'max_depth': [7,8, 9],\n",
    "    #'min_child_weight': [2,2.1,2.2,3,3.2]\n",
    "}#{'max_depth': 8, 'min_child_weight': 2.1}\n",
    "\n",
    "\n",
    "gsearch2 = GridSearchCV(\n",
    "    estimator=XGBRegressor(\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=140,\n",
    "        max_depth=5,\n",
    "        min_child_weight=2.1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective='reg:squarederror',\n",
    "        nthread=4,\n",
    "        scale_pos_weight=1,\n",
    "        seed=27\n",
    "    ),\n",
    "    param_grid=param_test2,\n",
    "    scoring='neg_mean_squared_error',  # Changed to a regression scoring metric\n",
    "    n_jobs=4,\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "gsearch2.fit(train_x4, train_y4)\n",
    "\n",
    "print(\"Best Parameters:\")\n",
    "print(gsearch2.best_params_)#new .5238"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OF9RguZuRYCF"
   },
   "source": [
    "### Tunning Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8aHX52uGMoxe",
    "outputId": "4f45236a-624e-456a-8c57-790a028867ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      "{'gamma': 0.2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "param_test3 = {\n",
    "    'gamma': [i/10.0 for i in range(0, 5)]\n",
    "}\n",
    "\n",
    "gsearch3 = GridSearchCV(\n",
    "    estimator=XGBRegressor(\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=140,\n",
    "        max_depth=8,\n",
    "        min_child_weight=2.1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective='reg:squarederror',\n",
    "        nthread=4,\n",
    "        scale_pos_weight=1,\n",
    "        seed=27\n",
    "    ),\n",
    "    param_grid=param_test3,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=4,\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "gsearch3.fit(train_x4, train_y4)\n",
    "\n",
    "print(\"Best Parameters:\")\n",
    "print(gsearch3.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q6dPK_ZVwBN7",
    "outputId": "93090121-e1df-4359-8be1-ecc91f2fcf4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      "{'gamma': 0.23}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "param_test3 = {\n",
    "    'gamma': [0.2,0.21,0.22,0.23,0.24]\n",
    "}\n",
    "\n",
    "gsearch3 = GridSearchCV(\n",
    "    estimator=XGBRegressor(\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=140,\n",
    "        max_depth=8,\n",
    "        min_child_weight=2.1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective='reg:squarederror',\n",
    "        nthread=4,\n",
    "        scale_pos_weight=1,\n",
    "        seed=27\n",
    "    ),\n",
    "    param_grid=param_test3,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=4,\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "gsearch3.fit(train_x4, train_y4)\n",
    "\n",
    "print(\"Best Parameters:\")\n",
    "print(gsearch3.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_pWvicfSqZW"
   },
   "source": [
    "### Tune subsample and colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KinCRhuESrby",
    "outputId": "ce0356a7-ce5a-48e9-a470-84c1fbc8f399"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      "{'colsample_bytree': 0.8, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "param_test4 = {\n",
    "    'subsample': [i/10.0 for i in range(6, 10)],\n",
    "    'colsample_bytree': [i/10.0 for i in range(6, 10)]\n",
    "}\n",
    "\n",
    "gsearch4 = GridSearchCV(\n",
    "    estimator=XGBRegressor(\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=140,\n",
    "        max_depth=8,\n",
    "        min_child_weight=2.1,\n",
    "        gamma=0.23,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective='reg:squarederror',\n",
    "        nthread=4,\n",
    "        scale_pos_weight=1,\n",
    "        seed=27\n",
    "    ),\n",
    "    param_grid=param_test4,\n",
    "    scoring='neg_mean_squared_error',  # Appropriate scoring for regression\n",
    "    n_jobs=4,\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "gsearch4.fit(train_x2, train_y2)\n",
    "\n",
    "print(\"Best Parameters:\")\n",
    "print(gsearch4.best_params_) #0.5210\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z_SEaqJ5U6nX",
    "outputId": "73f4e075-955d-4ad5-a508-eb6963742997"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      "{'colsample_bytree': 0.7, 'subsample': 0.85}\n"
     ]
    }
   ],
   "source": [
    "param_test5 = {#超久\n",
    " 'subsample':[i/100.0 for i in range(70,90,5)],\n",
    " 'colsample_bytree':[i/100.0 for i in range(70,90,5)]\n",
    "}\n",
    "gsearch5 = GridSearchCV(\n",
    "    estimator=XGBRegressor(\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=140,\n",
    "        max_depth=8,\n",
    "        min_child_weight=2.1,\n",
    "        gamma=0.23,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective='reg:squarederror',\n",
    "        nthread=4,\n",
    "        scale_pos_weight=1,\n",
    "        seed=27\n",
    "    ),\n",
    "    param_grid=param_test5,\n",
    "    scoring='neg_mean_squared_error',  # Appropriate scoring for regression\n",
    "    n_jobs=4,\n",
    "    cv=5\n",
    ")\n",
    "gsearch5.fit(train_x2, train_y2)\n",
    "print(\"Best Parameters:\")\n",
    "print(gsearch5.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "M5qwdd3Jlb2W"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      "{'colsample_bytree': 0.7, 'subsample': 0.85}\n"
     ]
    }
   ],
   "source": [
    "param_test5 = {#超久\n",
    " 'subsample':[0.81,0.83,0.85,0.87],\n",
    " 'colsample_bytree':[0.6,0.65,0.67,0.7,0.75]\n",
    "}\n",
    "gsearch5 = GridSearchCV(\n",
    "    estimator=XGBRegressor(\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=140,\n",
    "        max_depth=8,\n",
    "        min_child_weight=2.1,\n",
    "        gamma=0.23,\n",
    "        subsample=0.85,\n",
    "        colsample_bytree=0.7,\n",
    "        objective='reg:squarederror',\n",
    "        nthread=4,\n",
    "        scale_pos_weight=1,\n",
    "        seed=27\n",
    "    ),\n",
    "    param_grid=param_test5,\n",
    "    scoring='neg_mean_squared_error',  # Appropriate scoring for regression\n",
    "    n_jobs=4,\n",
    "    cv=5\n",
    ")\n",
    "gsearch5.fit(train_x2, train_y2)\n",
    "print(\"Best Parameters:\")\n",
    "print(gsearch5.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDKum5AgVP2c"
   },
   "source": [
    "### Tuning regularization parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5BIXEH0rVSCK",
    "outputId": "b1d3b637-e2cc-4fff-cae3-9d3a754d9741"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      "{'reg_alpha': 1e-05}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "param_test6 = {\n",
    " 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "gsearch6 = GridSearchCV(\n",
    "    estimator=XGBRegressor(\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=140,\n",
    "        max_depth=8,\n",
    "        min_child_weight=2.1,\n",
    "        gamma=0.23,\n",
    "        subsample=0.85,\n",
    "        colsample_bytree=0.7,\n",
    "        objective='reg:squarederror',\n",
    "        nthread=4,\n",
    "        scale_pos_weight=1,\n",
    "        seed=27\n",
    "    ),\n",
    "    param_grid=param_test6,\n",
    "    scoring='neg_mean_squared_error',  # Appropriate scoring for regression\n",
    "    n_jobs=4,\n",
    "    cv=5\n",
    ")\n",
    "gsearch6.fit(train_x4, train_y4)\n",
    "print(\"Best Parameters:\")\n",
    "print(gsearch6.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NQIgMoBdb7wR",
    "outputId": "3066eb8b-21fa-44a9-e237-788026aabeca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      "{'reg_alpha': 1e-05}\n"
     ]
    }
   ],
   "source": [
    "param_test7 = {\n",
    " 'reg_alpha':[0,1e-9,1e-7,1e-5, 1e-6]\n",
    "}\n",
    "gsearch7 = GridSearchCV(\n",
    "    estimator=XGBRegressor(\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=140,\n",
    "        max_depth=8,\n",
    "        min_child_weight=2.1,\n",
    "        gamma=0.23,\n",
    "        subsample=0.85,\n",
    "        colsample_bytree=0.7,\n",
    "        objective='reg:squarederror',\n",
    "        nthread=4,\n",
    "        scale_pos_weight=1,\n",
    "        seed=27\n",
    "    ),\n",
    "    param_grid=param_test7,\n",
    "    scoring='neg_mean_squared_error',  # Appropriate scoring for regression\n",
    "    n_jobs=4,\n",
    "    cv=5\n",
    ")\n",
    "gsearch7.fit(train_x4, train_y4)\n",
    "print(\"Best Parameters:\")\n",
    "print(gsearch7.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NeXyZYx8VYun"
   },
   "source": [
    "### Reducing the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CiHzkcxJVali",
    "outputId": "44572f7d-fb6d-4653-aa3e-d67a6ba9a1a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      "{'learning_rate': 0.05}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "param_test8 = {\n",
    " 'learning_rate':[0.001,0.01,0.02, 0.05,0.7,0.9]\n",
    " 'n_estimators':[900,1000,2000,3000,3500,4000]\n",
    "}\n",
    "gsearch8 = GridSearchCV(\n",
    "    estimator=XGBRegressor(\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=140,\n",
    "        max_depth=7,\n",
    "        min_child_weight=5.3,\n",
    "        gamma=0.4,\n",
    "        subsample=0.95,\n",
    "        colsample_bytree=0.65,\n",
    "        objective='reg:squarederror',\n",
    "        nthread=4,\n",
    "        reg_alpha = 1e-05,\n",
    "        seed=27\n",
    "    ),\n",
    "    param_grid=param_test8,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    cv=5\n",
    ")\n",
    "gsearch8.fit(train_x4, train_y4)\n",
    "print(\"Best Parameters:\")\n",
    "print(gsearch8.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RFBZzYXoVa86",
    "outputId": "ee39d8aa-10d1-4a42-a3ed-e5fc4be76cac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      "{'n_estimators': 6500}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "param_test9 = {\n",
    "    'n_estimators':[3000,3500,4000,4500,5000,6500]\n",
    "}\n",
    "gsearch9 = GridSearchCV(\n",
    "    estimator=XGBRegressor(\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=140,\n",
    "        max_depth=7,\n",
    "        min_child_weight=5.3,\n",
    "        gamma=0.4,\n",
    "        subsample=0.95,\n",
    "        colsample_bytree=0.65,\n",
    "        objective='reg:squarederror',\n",
    "        nthread=4,\n",
    "        reg_alpha = 1e-05,\n",
    "        seed=27\n",
    "    ),\n",
    "    param_grid=param_test9,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    cv=5\n",
    ")\n",
    "gsearch9.fit(train_x4, train_y4)\n",
    "print(\"Best Parameters:\")\n",
    "print(gsearch9.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ue2iF8zOClFS",
    "outputId": "65f58b92-03a1-43c2-f8af-771f810f40b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      "{'n_estimators': 4500}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "param_test10 = {\n",
    "    'n_estimators':[4450,4500,4550]\n",
    "}\n",
    "gsearch10 = GridSearchCV(\n",
    "    estimator=XGBRegressor(\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=140,\n",
    "        max_depth=7,\n",
    "        min_child_weight=5.3,\n",
    "        gamma=0.4,\n",
    "        subsample=0.95,\n",
    "        colsample_bytree=0.65,\n",
    "        objective='reg:squarederror',\n",
    "        nthread=4,\n",
    "        reg_alpha = 1e-05,\n",
    "        seed=27\n",
    "    ),\n",
    "    param_grid=param_test10,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    cv=5\n",
    ")\n",
    "gsearch10.fit(train_x4, train_y4)\n",
    "print(\"Best Parameters:\")\n",
    "print(gsearch10.best_params_)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
